[
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/view-manage-runs",
    "type": "Public Preview",
    "description": "This article describes how to view, manage, and analyzeFoundation Model Fine-tuning(now part of Mosaic AI Model Training) runs using APIs or using the UI.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-26",
    "ai_summary": "Manage and analyze Foundation Model Fine-tuning runs using APIs or UI, with filtering options for users and timestamps.",
    "is_new": true,
    "title": "View, manage, and analyze Foundation Model Fine-tuning runs"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/human-evaluation",
    "type": "Public Preview",
    "description": "Databricks recommends you use the currentReview Appversion.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-28",
    "ai_summary": "Gather human feedback on AI agent quality using Databricks Review App, enabling conversational reviews and inference table analysis.",
    "is_new": true,
    "title": "Get feedback about the quality of an agentic application (MLflow 2)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/resignal-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "SIGNAL statement re-raises handled condition within compound statements, available in Public Preview for Databricks SQL and Runtime 16.3+.",
    "is_new": true,
    "title": "RESIGNAL statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/",
    "type": "Public Preview",
    "description": "Databricksmakes it simple to access and build off of publicly available large language models.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-11-15",
    "ai_summary": "Access and fine-tune large language models using Hugging Face Transformers, LangChain, and Foundation Model Fine-tuning on Databricks.",
    "is_new": true,
    "title": "Large language models (LLMs) on Databricks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/while-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Repeat SQL statements while a condition is true, with optional label for looping and conditional execution.",
    "is_new": true,
    "title": "WHILE statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/tables/managed",
    "type": "Public Preview",
    "description": "This page describesUnity Catalogmanaged tables inDelta LakeandApache Iceberg, the default and recommended table type inDatabricks. These tables are fully governed and optimized byUnity Catalog, offering performance, operational advantages, and lower storage and compute costs compared toexternalandforeign tables, because managed tables learn from your read and write pattern.Unity Catalogmanages all read, write, storage, and optimization responsibilities for managed tables.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-11",
    "ai_summary": "Unity Catalog managed tables in Delta Lake and Apache Iceberg offer optimized storage, query performance, and security features.",
    "is_new": true,
    "title": "Unity Catalog managed tables in Databricks for Delta Lake and Apache Iceberg"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/ai-playground",
    "type": "Public Preview",
    "description": "The AI Playground is a chat environment for Large Language Models (LLMs).\nIn the playground, you can interactively test, prompt, and compare LLMs, as well as prototype tool-calling agents and question-answering bots.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-28",
    "ai_summary": "AI Playground allows interactive testing and prototyping of Large Language Models, with features for comparing responses and building AI agents.",
    "is_new": true,
    "title": "Chat with LLMs and prototype generative AI apps using AI Playground"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/query/psql",
    "type": "Public Preview",
    "description": "This page describes how to access a Lakebase database instance from SQL clients and walks through how to connect to a database instance frompsql.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Access Lakebase database instances using SQL clients like psql, DBeaver, and pgAdmin; requires psql version 14 or higher.",
    "is_new": true,
    "title": "Access a database instance from SQL clients"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/dbsql/materialized-monitor",
    "type": "Public Preview",
    "description": "This article describes how to monitor and query refresh data about amaterialized viewinDatabricks SQL.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-21",
    "ai_summary": "Monitor and query materialized view refresh data in Databricks SQL, including logs, status, and storage location.",
    "is_new": true,
    "title": "Monitor materialized views in Databricks SQL"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_fix_grammar",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "A state-of-the-art AI model corrects grammatical errors in text using SQL, available on certain workspaces and notebooks.",
    "is_new": true,
    "title": "ai_fix_grammar function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/column_masks",
    "type": "Public Preview",
    "description": "Databricks Runtime12.2 LTS and aboveUnity Catalogonly.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-18",
    "ai_summary": "Databricks Runtime 12.2+ introduces Unity Catalog's INFORMATION_SCHEMA.COLUMN_MASKS, displaying column masking metadata for tables and masks.",
    "is_new": true,
    "title": "COLUMN_MASKS"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-drop-procedure",
    "type": "Public Preview",
    "description": "Applies to:Databricks Runtime17.0 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Drops a user-defined procedure, requiring `MANAGE` privilege or ownership, with optional `IF EXISTS` exception handling.",
    "is_new": true,
    "title": "DROP PROCEDURE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/udf/unity-catalog",
    "type": "Public Preview",
    "description": "User-defined functions (UDFs) inUnity Catalogextend SQL and Python's capabilities withinDatabricks. They allow custom functions to be defined, used, and securely shared and governed across computing environments.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-04",
    "ai_summary": "User-defined functions (UDFs) extend SQL and Python capabilities, enabling custom function definition, sharing, and governance across environments.",
    "is_new": true,
    "title": "User-defined functions (UDFs) in Unity Catalog"
  },
  {
    "url": "https://docs.databricks.com/aws/en/databricks-ai/databricks-ai-trust",
    "type": "Public Preview",
    "description": "Databricks understands the importance of your data and the trust you place in us when you use our platform and Databricks AI features. Databricks is committed to the highest standards of data protection, and has implemented rigorous measures to ensure information you submit to Databricks AI features is protected.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "Databricks prioritizes data protection, ensuring confidentiality, zero data retention, and rigorous measures to safeguard user information.",
    "is_new": true,
    "title": "Databricks AI features trust and safety"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/table_constraints",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-18",
    "ai_summary": "Databricks introduces INFORMATION_SCHEMA.TABLE_CONSTRAINTS, a feature showing primary and foreign key constraints in Unity Catalog, with column descriptions.",
    "is_new": true,
    "title": "TABLE_CONSTRAINTS"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/routine_privileges",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-13",
    "ai_summary": "View routine privileges with INFORMATION_SCHEMA.ROUTINE_PRIVILEGES, showing principals granting or inheriting rights on specific routines in Unity Catalog.",
    "is_new": true,
    "title": "ROUTINE_PRIVILEGES"
  },
  {
    "url": "https://docs.databricks.com/aws/en/partners/ingestion/qlik",
    "type": "Public Preview",
    "description": "Qlik Replicatehelps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe and more) intoDelta Lake. Replicate's automated change data capture (CDC) helps you avoid the heavy lifting of manually extracting data, transferring using an API script, chopping, staging, and importing. Qlik Compose automates the CDC intoDelta Lake.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-02-28",
    "ai_summary": "Qlik Replicate integrates data from multiple sources into Delta Lake using automated change data capture and OAuth tokens.",
    "is_new": true,
    "title": "Connect to Qlik Replicate"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta/generated-columns",
    "type": "Public Preview",
    "description": "Delta Lakesupports generated columns which are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table. When you write to a table with generated columns and you do not explicitly provide values for them,Delta Lakeautomatically computes the values. For example, you can automatically generate a date column (for partitioning the table by date) from the timestamp column; any writes into the table need only specify the data for the timestamp column. However, if you explicitly provide values for them, the values must satisfy theconstraint(<value> <=> <generation expression>) IS TRUEor the write will fail with an error.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-11-01",
    "ai_summary": "Delta Lake supports generated columns, automatically computing values based on user-specified functions over other table columns.",
    "is_new": true,
    "title": "Delta Lake generated columns"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/evaluating-production-traffic",
    "type": "Public Preview",
    "description": "This article describes an older product with limited functionality. Databricks recommends that you useWhat is Lakehouse Monitoring for generative AI? (MLflow 2)instead.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-02",
    "ai_summary": "Monitor deployed AI agents' quality on production traffic using Mosaic AI Agent Evaluation, enabling dashboard customization and error debugging.",
    "is_new": true,
    "title": "How to monitor the quality of your agent on production traffic"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/serverless-dlt",
    "type": "Public Preview",
    "description": "This article describes configurations for serverlessLakeflow Declarative Pipelines.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-18",
    "ai_summary": "Configure serverless Lakeflow Declarative Pipelines with Unity Catalog, no compute settings allowed, and optional source code and notebook setup.",
    "is_new": true,
    "title": "Configure a serverless pipeline"
  },
  {
    "url": "https://docs.databricks.com/aws/en/security/network/serverless-network-security/pl-to-internal-network",
    "type": "Public Preview",
    "description": "Effective October 7, 2024,Databricksbegan charging customers for networking costs incurred from serverless compute resources connecting to external resources. Serverless network billing is rolling out in phases, which might result in gradual billing changes. For more information on billing, seeUnderstand Databricks serverless networking costs.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-04",
    "ai_summary": "Databricks introduces serverless networking costs; configure PrivateLink connections from serverless compute to VPC resources through network load balancers.",
    "is_new": true,
    "title": "Configure private connectivity to resources in your VPC"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/register-uc",
    "type": "Public Preview",
    "description": "This page explains how to register your Lakebase database as a read-onlyUnity Catalogcatalog. This allows you to useUnity Catalogprivileges to manage data access and leverage integrations like managed data syncing.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Register a Lakehouse database as a read-only Unity Catalog to manage data access and leverage integrations like managed syncing.",
    "is_new": true,
    "title": "Register your database in Unity Catalog"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring-features",
    "type": "Beta",
    "description": "This page describes usage of Agent Evaluation version0.22with MLflow 2. Databricks recommends using MLflow 3, which is integrated with Agent Evaluation>1.0. In MLflow 3, Agent Evaluation APIs are now part of themlflowpackage.For information on this topic, seeProduction quality monitoring (running scorers automatically).",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-03",
    "ai_summary": "Monitor AI/ML model quality with Agent Evaluation, viewing results in Lakehouse Monitoring after setting up experiments and SQL warehouse.",
    "is_new": true,
    "title": "Monitor generative AI apps (MLflow 2)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/user/sql-editor/custom-format",
    "type": "Public Preview",
    "description": "This article explains how to customize SQL auto-formatting options in theDatabricksUI.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-02",
    "ai_summary": "Configure SQL formatting options in Databricks to improve readability and maintainability with customizable indentation, casing, and more.",
    "is_new": true,
    "title": "Custom format SQL statements"
  },
  {
    "url": "https://docs.databricks.com/aws/en/genie/best-practices",
    "type": "Public Preview",
    "description": "The goal of curating a Genie space is to create an environment where business users can pose natural language questions and receive accurate, consistent answers based on their data. Genie spaces use advanced models that generate sophisticated queries and understand general world knowledge.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-29",
    "ai_summary": "Curate Genie spaces by starting small, focusing on essential data, and iteratively refining instructions for accurate business question answers.",
    "is_new": true,
    "title": "Curate an effective Genie space"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/tag-policies/manage-permissions",
    "type": "Beta",
    "description": "This page explains how to create and manage tag policies permissions. For an overview of tag policies, seeTag policies.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-09",
    "ai_summary": "Create and manage tag policy permissions, defining who can create, edit, assign, or delete policies at account or individual levels.",
    "is_new": true,
    "title": "Manage tag policy permissions"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta/variant",
    "type": "Public Preview",
    "description": "You can use theVARIANTdata type to store semi-structured data inDelta Lake. For examples on working withVARIANT, seeQuery variant data.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-03",
    "ai_summary": "Introduce VARIANT data type to store semi-structured data in Delta Lake, with limitations and requirements for read/write support.",
    "is_new": true,
    "title": "Variant support in Delta Lake"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dashboards/git-support",
    "type": "Public Preview",
    "description": "This article explains how to use Databricks Git folders for version control and collaborative dashboard development. It also provides guidance on implementing CI/CD processes to develop and deploy dashboards across different workspaces.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-09",
    "ai_summary": "Use Databricks Git folders for collaborative dashboard development and version control, simplifying deployment and recovery with centralized tracking.",
    "is_new": true,
    "title": "Source control dashboards"
  },
  {
    "url": "https://docs.databricks.com/aws/en/lakehouse-monitoring/data-classification",
    "type": "Beta",
    "description": "Data catalogs can have a vast amount of data, often containing known and unknown sensitive data.\nIt is critical for data teams to understand what kind of sensitive data exists in each table so that they can both govern and democratize access to this data.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-21",
    "ai_summary": "Databricks Data Classification automatically tags and classifies sensitive data in Unity Catalog for governance and democratization control.",
    "is_new": true,
    "title": "Data Classification"
  },
  {
    "url": "https://docs.databricks.com/aws/en/genie/",
    "type": "Public Preview",
    "description": "This page introduces AI/BI Genie,a Databricksfeature that allows business teams to interact with their data using natural language. It uses generative AI tailored to your organization's terminology and data, with the ability to monitor and refine its performance through user feedback.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Databricks AI/BI Genie enables business teams to interact with data using natural language, generating visualizations and answering questions.",
    "is_new": true,
    "title": "What is an AI/BI Genie space"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/machine-learning/llm-optimized-model-serving",
    "type": "Public Preview",
    "description": "The code examples in this guide use deprecated APIs. Databricks recommends using theprovisioned throughputexperience for optimized inference of LLMs. SeeMigrate optimized LLM serving endpoints to provisioned throughput.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-30",
    "ai_summary": "Optimize large language models (LLMs) on Mosaic AI Model Serving for improved throughput and latency, using provisioned throughput.",
    "is_new": true,
    "title": "Optimized large language model (LLM) serving"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/user/sql-editor/",
    "type": "Public Preview",
    "description": "The Databricks UI includes a SQL editor that you can use to author queries, collaborate with colleagues, browse available data, and create visualizations. This page explains how to use the SQL editor to write, run, manage, and share queries.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Explore and manage SQL queries using Databricks' new SQL editor, including collaboration, visualization, and sharing features.",
    "is_new": true,
    "title": "Write queries and explore data in the new SQL editor"
  },
  {
    "url": "https://docs.databricks.com/aws/en/jobs/configure-job",
    "type": "Public Preview",
    "description": "You can create and run a job using the Jobs UI, or developer tools such as the Databricks CLI or the REST API. Using the UI or API, you can repair and rerun a failed or canceled job. This article shows how to create, configure, and edit jobs using theJobs & Pipelinesworkspace UI. For information about other tools, see the following:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-18",
    "ai_summary": "Create and manage jobs using the UI, CLI, or API; configure compute resources, schedules, and tasks for notebook-based workflows.",
    "is_new": true,
    "title": "Configure and edit Lakeflow Jobs"
  },
  {
    "url": "https://docs.databricks.com/aws/en/stateful-applications",
    "type": "Public Preview",
    "description": "You can build streaming applications using custom stateful operators to implement low-latency and near real-time solutions that use arbitrary stateful logic. Custom stateful operators unlock new operational use cases and patterns unavailable through traditionalStructured Streamingprocessing.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-14",
    "ai_summary": "Custom stateful operators enable low-latency streaming applications with arbitrary logic, ideal for use cases unavailable through traditional Structured Streaming.",
    "is_new": true,
    "title": "Build a custom stateful application"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/manage-privileges",
    "type": "Public Preview",
    "description": "This page describes how identities, roles and permissions work when using Lakebase.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Lakebase identities, roles, and permissions: automatic Postgres role setup for creators, with varying access controls depending on usage scenarios.",
    "is_new": true,
    "title": "Database roles, access, and privileges"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/constraint_table_usage",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-22",
    "ai_summary": "Retrieve all constraints referencing a table as foreign or primary key in Unity Catalog using INFORMATION_SCHEMA.CONSTRAINT_TABLE_USAGE.",
    "is_new": true,
    "title": "CONSTRAINT_TABLE_USAGE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_gen",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "Databricks' AI Function, `ai_gen()`, generates answers to user prompts using state-of-the-art generative AI models in English.",
    "is_new": true,
    "title": "ai_gen function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/data-governance/unity-catalog/abac/policies",
    "type": "Beta",
    "description": "This page describes how to configure row filter and column mask policies inUnity Catalog. For more information on attribute-based access control (ABAC) and policies, seeUnity Catalogattribute-based access control (ABAC). To apply tags to objects, seeTag policiesandApply tags toUnity Catalogsecurable objects.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-10",
    "ai_summary": "Configure row filter and column mask policies in Unity Catalog using Attribute-Based Access Control (ABAC) beta feature.",
    "is_new": true,
    "title": "Create and manage attribute-based access control (ABAC) policies"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_translate",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "Databricks introduces AI-powered text translation feature, translating English to Spanish during public preview.",
    "is_new": true,
    "title": "ai_translate function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/model-serving/enable-model-serving-inference-tables",
    "type": "Public Preview",
    "description": "This article describes the legacy inference table experience which is only relevant for certain provisioned throughput and custom model endpoints. This experience is not recommended. Databricks recommendsAI Gateway-enabled inference tablesfor its availability on custom model, foundation model and agent serving endpoints.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-13",
    "ai_summary": "Enable inference tables for custom model serving endpoints using API; requires Unity Catalog, specific permissions, and catalog/schema settings.",
    "is_new": true,
    "title": "Enable inference tables on model serving endpoints using the API"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/create/monitor",
    "type": "Public Preview",
    "description": "This page explains how to monitor the performance of your Lakebase database instance using theMetricstab on the instance details page.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Monitor Lakebase database performance using metrics on the instance details page, analyzing transactions, rows, and open connections.",
    "is_new": true,
    "title": "Monitor a database instance"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/schema_of_xml",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime14.1 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-02-01",
    "ai_summary": "This feature returns the schema of an XML string in DDL format, with optional mapping options.",
    "is_new": true,
    "title": "schema_of_xml function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring-non-agent-framework",
    "type": "Beta",
    "description": "This page describes usage of Agent Evaluation version0.22with MLflow 2. Databricks recommends using MLflow 3, which is integrated with Agent Evaluation>1.0. In MLflow 3, Agent Evaluation APIs are now part of themlflowpackage.For information on this topic, seeProduction quality monitoring (running scorers automatically).",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-03",
    "ai_summary": "Set up monitoring for generative AI apps outside Databricks using MLflow and Mosaic AI Agent Evaluation with Databricks Agents.",
    "is_new": true,
    "title": "Monitor apps deployed outside of Databricks (MLflow 2)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/ingestion/variant",
    "type": "Public Preview",
    "description": "InDatabricks Runtime15.3 and above, you can use theVARIANTtype to ingest semi-structured data. This article describes behavior and provides example patterns for ingesting data from cloud object storage usingAuto LoaderandCOPY INTO, streaming records from Kafka, and SQL commands for creating new tables with variant data or inserting new records using the variant type. The following table summarizes the supported file formats andDatabricks Runtimeversion support:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-11",
    "ai_summary": "Databricks introduces VARIANT type for semi-structured data ingestion from cloud storage, Kafka, and SQL commands starting with Runtime 15.3.",
    "is_new": true,
    "title": "Ingest data as semi-structured variant type"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt-ref/dlt-sql-ref-create-streaming-table",
    "type": "Public Preview",
    "description": "Astreaming tableis a table with support for streaming or incremental data processing.Streaming tablesare backed byLakeflow Declarative Pipelines. Each time anstreaming tableis refreshed, data added to the source tables is appended to thestreaming table. You can refreshstreaming tablesmanually or on a schedule.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-08",
    "ai_summary": "Create a streaming table for incremental data processing with Lakeflow Pipelines, specifying options like refresh and private access.",
    "is_new": true,
    "title": "CREATE STREAMING TABLE (Lakeflow Declarative Pipelines)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow/model-registry-webhooks",
    "type": "Public Preview",
    "description": "Webhooks enable you to listen for Workspace Model Registry events so your integrations can automatically trigger actions. You can use webhooks to automate and integrate your machine learning pipeline with existing CI/CD tools and workflows. For example, you can trigger CI builds when a new model version is created or notify your team members through Slack each time a model transition to production is requested.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-11",
    "ai_summary": "Webhooks enable automated actions in machine learning pipelines, triggered by events such as model version creation or transitions.",
    "is_new": true,
    "title": "Workspace Model Registry webhooks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/manage-model-lifecycle/workspace-model-registry",
    "type": "Public Preview",
    "description": "This documentation covers the Workspace Model Registry. If your workspace is enabled forUnity Catalog, do not use the procedures on this page. Instead, seeModels in Unity Catalog.For guidance on how to upgrade from the Workspace Model Registry toUnity Catalog, seeMigrate workflows and models toUnity Catalog.If your workspace'sdefault catalogis inUnity Catalog(rather thanhive_metastore), and you are either running a cluster usingDatabricks Runtime13.3 LTS or above or usingMLflow 3, models are automatically created in and loaded from the workspace default catalog, with no configuration required. To use the Workspace Model Registry in this case, you must explicitly target it by runningimport mlflow; mlflow.set_registry_uri(\"databricks\")at the start of your workload. A small number of workspaces where both the default catalog was configured to a catalog inUnity Catalogprior to January 2024 and the workspace model registry was used prior to January 2024 are exempt from this behavior and continue to use the Workspace Model Registry by default.Starting in April 2024, Databricks disabled Workspace Model Registry for workspaces in new accounts where the workspace'sdefault catalogis inUnity Catalog.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-12",
    "ai_summary": "Workspace Model Registry deprecated; use Unity Catalog instead, with automatic model creation for some workspaces.",
    "is_new": true,
    "title": "Manage model lifecycle using the Workspace Model Registry (legacy)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/tag-policies/",
    "type": "Beta",
    "description": "This page provides an overview of tag policies inDatabricks. To create and manage tag policies, seeCreate and manage tag policies. To apply tags, seeApply tags toUnity Catalogsecurable objects.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-09",
    "ai_summary": "Databricks tag policies govern metadata tagging, ensuring consistency, security, and compliance by controlling tag keys, values, and assignments.",
    "is_new": true,
    "title": "Tag policies"
  },
  {
    "url": "https://docs.databricks.com/aws/en/jobs/run-serverless-jobs",
    "type": "Public Preview",
    "description": "Serverless compute for workflowsallows you to run your job without configuring and deploying infrastructure. Withserverless compute, you focus on implementing your data processing and analysis pipelines, andDatabricksefficiently manages compute resources, including optimizing and scaling compute for your workloads. Autoscaling andPhotonare automatically enabled for the compute resources that run your job.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-18",
    "ai_summary": "Serverless compute for workflows enables running jobs without infrastructure setup, optimizing resources for efficient processing and scaling.",
    "is_new": true,
    "title": "Run your Lakeflow Jobs with serverless compute for workflows"
  },
  {
    "url": "https://docs.databricks.com/aws/en/stateful-applications/",
    "type": "Public Preview",
    "description": "You can build streaming applications using custom stateful operators to implement low-latency and near real-time solutions that use arbitrary stateful logic. Custom stateful operators unlock new operational use cases and patterns unavailable through traditionalStructured Streamingprocessing.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-14",
    "ai_summary": "Build custom stateful streaming applications with low-latency and near real-time solutions using transformWithState operator in Databricks Runtime 16.2+.",
    "is_new": true,
    "title": "Build a custom stateful application"
  },
  {
    "url": "https://docs.databricks.com/aws/en/lakehouse-monitoring/data-quality-monitoring",
    "type": "Beta",
    "description": "This page describes what data quality monitoring is, what it monitors, and how to use it. Data quality monitoring was formerly called anomaly detection.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-10",
    "ai_summary": "Databricks' Data Quality Monitoring feature tracks freshness and completeness for all tables in a schema, providing insights and alerts.",
    "is_new": true,
    "title": "Data quality monitoring"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis/deploy-prov-throughput-foundation-model-apis",
    "type": "Public Preview",
    "description": "This article demonstrates how to deploy models usingFoundation Model APIsprovisioned throughput. Databricks recommends provisioned throughput for production workloads, and it provides optimized inference for foundation models with performance guarantees.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-30",
    "ai_summary": "Deploy Foundation Models using provisioned throughput on Databricks, with optimized inference and performance guarantees for production workloads.",
    "is_new": true,
    "title": "Provisioned throughput Foundation Model APIs"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/dbsql/streaming",
    "type": "Public Preview",
    "description": "Databricks recommends usingstreaming tablesto ingest data usingDatabricks SQL. Astreaming tableis a table registered toUnity Catalogwith extra support for streaming or incremental data processing. A pipeline is automatically created for eachstreaming table. You can usestreaming tablesfor incremental data loading from Kafka and cloud object storage.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-21",
    "ai_summary": "Use streaming tables in Databricks SQL for incremental data processing with Kafka and cloud storage, meeting workspace and compute requirements.",
    "is_new": true,
    "title": "Use streaming tables in Databricks SQL"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-20",
    "ai_summary": "Defines a managed or external table with options to replace, create IF NOT EXISTS, and specify location.",
    "is_new": true,
    "title": "CREATE TABLE [USING]"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/reference",
    "type": "Beta",
    "description": "The MLflow Prompt Registry is a centralized repository for managing prompt templates across their lifecycle. It enables teams to:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-18",
    "ai_summary": "MLflow Prompt Registry: centralize and manage prompt templates, version control, deployment, collaboration, governance, and lineage tracking.",
    "is_new": true,
    "title": "Prompt Registry reference"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta-sharing/read-data-open",
    "type": "Public Preview",
    "description": "This page describes how to read data shared with you using theDelta Sharingopen sharingprotocol with bearer tokens. It includes instructions for reading shared data using the following tools:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-15",
    "ai_summary": "Read shared data with bearer tokens using Apache Spark, Databricks, pandas, and Power BI with credential file access.",
    "is_new": true,
    "title": "Read data shared using Delta Sharing open sharing with bearer tokens (for recipients)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta/tutorial",
    "type": "Public Preview",
    "description": "This tutorial introduces commonDelta Lakeoperations onDatabricks, including the following:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-30",
    "ai_summary": "Learn common Delta Lake operations on Databricks, including creating and managing tables, indexes, and data with Python, Scala, and SQL.",
    "is_new": true,
    "title": "Tutorial: Delta Lake"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/get-diagnostics-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Retrieve information about an exception handler, including condition, message text, SQLSTATE, and arguments, in a public preview feature.",
    "is_new": true,
    "title": "GET DIAGNOSTICS statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/collation",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.1 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Returns the collation attached to an ASTRING expression using default collation in Databricks SQL and Runtime 16.1 and above.",
    "is_new": true,
    "title": "collation function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-aux-show-tables-dropped",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime12.2 LTS and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-18",
    "ai_summary": "Lists dropped tables in Unity Catalog, filterable by schema name and max result, with detailed information on each table.",
    "is_new": true,
    "title": "SHOW TABLES DROPPED"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/data-preparation",
    "type": "Public Preview",
    "description": "This article describes the accepted training and evaluation data file formats forFoundation Model Fine-tuning(now part of Mosaic AI Model Training).",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-11",
    "ai_summary": "Foundation Model Fine-tuning requires specific data formats for chat completion and continued pre-training, with guidelines for validation and formatting.",
    "is_new": true,
    "title": "Prepare data for Foundation Model Fine-tuning"
  },
  {
    "url": "https://docs.databricks.com/aws/en/error-messages/",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime12.2 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-10",
    "ai_summary": "Databricks error components include Error Condition, SQLSTATE, Parameterized Message, and Message Parameters, providing detailed error information.",
    "is_new": true,
    "title": "Error handling in Databricks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt-ref/dlt-sql-ref-create-materialized-view",
    "type": "Public Preview",
    "description": "Amaterialized viewis a view where precomputed results are available for query and can be updated to reflect changes in the input.materialized viewsare backed byLakeflow Declarative Pipelines. Each time amaterialized viewis updated, query results are recalculated to reflect changes in upstream datasets. You can updatedmaterialized viewsmanually or on a schedule.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-08",
    "ai_summary": "Materialized views store precomputed results, updating when input datasets change, with options for manual or scheduled updates and private views.",
    "is_new": true,
    "title": "CREATE MATERIALIZED VIEW (Lakeflow Declarative Pipelines)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-aux-call",
    "type": "Public Preview",
    "description": "Applies to:Databricks Runtime17.0 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Invoke stored procedures by name, passing arguments and handling OUT/INOUT parameters with up to 64 levels of nesting.",
    "is_new": true,
    "title": "CALL"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/evaluating-production-traffic-fm",
    "type": "Public Preview",
    "description": "This article describes an older product with limited functionality. Databricks recommends that you useWhat is Lakehouse Monitoring for generative AI? (MLflow 2)instead.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-02",
    "ai_summary": "Monitor external models' quality on production traffic using AI Gateway, enabling real-time dashboarding and customizable analytics.",
    "is_new": true,
    "title": "How to monitor the quality of your external model on production traffic"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/train-model/sparkdl-xgboost",
    "type": "Public Preview",
    "description": "sparkdl.xgboostis deprecated starting withDatabricks Runtime12.0 ML, and is removed inDatabricks Runtime13.0 ML and above. For information about migrating your workloads toxgboost.spark, seeMigration guide for the deprecatedsparkdl.xgboostmodule.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-06-26",
    "ai_summary": "XGBoost for PySpark deprecated; migrate to xgboost.spark module; note limitations and changes in parameters and distributed training.",
    "is_new": true,
    "title": "Distributed training of XGBoost models using sparkdl.xgboost"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/vector_search",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQL",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Searches a Mosaic AI Vector Search index using SQL, with options for querying text or vectors in Databricks Runtime 15.3+.",
    "is_new": true,
    "title": "vector_search function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/create/capacity",
    "type": "Public Preview",
    "description": "This page explains the options for right-sizing your Lakebase instance capacity and how to manage it.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-11",
    "ai_summary": "Resize Lakehouse instance capacity by selecting new size from Capacity menu, considering RAM and CPU resources for optimal performance.",
    "is_new": true,
    "title": "Manage instance capacity"
  },
  {
    "url": "https://docs.databricks.com/aws/en/query-federation/oracle",
    "type": "Public Preview",
    "description": "This article describes how to set up Lakehouse Federation to run federated queries onOracledata that is not managed byDatabricks. To learn more about Lakehouse Federation, seeWhat is Lakehouse Federation?.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-15",
    "ai_summary": "Set up Lakehouse Federation to run federated queries on Oracle data not managed by Databricks with Unity Catalog metastore.",
    "is_new": true,
    "title": "Run federated queries on Oracle"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_parse_document",
    "type": "Beta",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-25",
    "ai_summary": "Databricks introduces AI-powered document parsing with Llama models, available in notebooks, SQL editor, and workflows, with beta pricing.",
    "is_new": true,
    "title": "ai_parse_document function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta/type-widening",
    "type": "Public Preview",
    "description": "Tables with type widening enabled allow you to change column data types to a wider type without rewriting underlying data files. You can either change column types manually or use schema evolution to evolve column types.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-03",
    "ai_summary": "Enable type widening in Databricks Runtime 15.4 LTS+ to change column data types without rewriting underlying files.",
    "is_new": true,
    "title": "Type widening"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_generate_text",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQL",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-03-01",
    "ai_summary": "Generate text using large language models (LLM) like GPT-3.5-turbo or Azure OpenAI, with optional parameters for temperature and stop strings.",
    "is_new": true,
    "title": "ai_generate_text function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/genie/conversation-api",
    "type": "Public Preview",
    "description": "This page explains how to use the Genie Conversation API to enable Genie capabilities in your own chatbot, agent, or application.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Use Genie Conversation API to integrate natural language querying into applications, chatbots, and AI agents with curated Databricks spaces.",
    "is_new": true,
    "title": "Use the Genie Conversation API to integrate Genie into your applications"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/runtime-release-notes/5.4ml",
    "type": "Public Preview",
    "description": "Support for thisDatabricks Runtimeversion has ended. For the end-of-support date, seeEnd-of-support history. For all supportedDatabricks Runtimeversions, seeDatabricks Runtimerelease notes versions and compatibility.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-26",
    "ai_summary": "Databricks Runtime 5.4 Machine Learning supports popular libraries, Hyperopt, Apache Spark MLlib, and Horovod with automated MLflow tracking.",
    "is_new": true,
    "title": "Databricks Runtime 5.4 for ML (EoS)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/routine_columns",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-18",
    "ai_summary": "Databricks SQL feature in public preview: Information Schema Routines and Columns for table-valued functions.",
    "is_new": true,
    "title": "ROUTINE_COLUMNS"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/legacy-model-serving/model-serving",
    "type": "Public Preview",
    "description": "Legacy MLflow Model Serving allows you to host machine learning models from Model Registry as REST endpoints that are updated automatically based on the availability of model versions and their stages. It uses a single-node cluster that runs under your own account within what is now called the classic compute plane. This compute plane includes the virtual network and its associated compute resources such as clusters for notebooks and jobs, pro and classic SQL warehouses, and Legacy model serving endpoints.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-10-30",
    "ai_summary": "Legacy MLflow Model Serving hosts machine learning models as REST endpoints, automating updates and deployment from Model Registry.",
    "is_new": true,
    "title": "Legacy MLflow Model Serving on Databricks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/partners/ingestion/syncsort",
    "type": "Public Preview",
    "description": "Syncsorthelps you break down data silos by integrating legacy, mainframe, and IBM data withDatabricks. You can easily pull data from these sources intoDelta Lake.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2023-10-10",
    "ai_summary": "Databricks Syncsort integrates legacy data sources with Delta Lake, using personal access tokens and secure S3 bucket access.",
    "is_new": true,
    "title": "Connect to Syncsort"
  },
  {
    "url": "https://docs.databricks.com/aws/en/compute/simple-form",
    "type": "Public Preview",
    "description": "This article explains how to access and use the new simplified compute UI to create and edit all-purpose and job compute.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-08",
    "ai_summary": "Databricks simplifies compute UI with updated settings, including policy selection, machine learning runtime, and access mode options for easier use.",
    "is_new": true,
    "title": "Use the simple form to manage compute"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/account-settings/audit-logs",
    "type": "Beta",
    "description": "This feature requires thePremium plan or above.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-20",
    "ai_summary": "Databricks audit logs provide detailed usage patterns, logging various services, events, and activities for monitoring and compliance purposes.",
    "is_new": true,
    "title": "Audit log reference"
  },
  {
    "url": "https://docs.databricks.com/aws/en/ai-gateway/",
    "type": "Public Preview",
    "description": "This article describes Mosaic AI Gateway, the Databricks solution for governing and monitoring access to supported generative AI models and their associated model serving endpoints.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-11",
    "ai_summary": "Mosaic AI Gateway governs, monitors, and manages generative AI models, agents, and endpoints for streamlined usage and production readiness.",
    "is_new": true,
    "title": "Mosaic AI Gateway introduction"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_query",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-15",
    "ai_summary": "Databricks AI Query feature: invokes existing model serving endpoints, parsing responses; requires specific runtime and permissions.",
    "is_new": true,
    "title": "ai_query function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_analyze_sentiment",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "Analyzes input text sentiment using AI model, returns positive/negative/neutral/mixed results, with licenses and requirements for usage.",
    "is_new": true,
    "title": "ai_analyze_sentiment function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/ai-gateway/configure-ai-gateway-endpoints",
    "type": "Public Preview",
    "description": "In this article, you learn how to configureMosaic AI Gatewayon a model serving endpoint.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Configure Mosaic AI Gateway on a model serving endpoint, enabling usage tracking and payload logging features using the Serving UI",
    "is_new": true,
    "title": "Configure AI Gateway on model serving endpoints"
  },
  {
    "url": "https://docs.databricks.com/aws/en/compute/serverless/gpu",
    "type": "Beta",
    "description": "This article describes serverless GPU compute on Databricks and provides recommended use cases, guidance for how to set up GPU compute resources, and feature limitations.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-30",
    "ai_summary": "Databricks offers serverless GPU compute for custom deep learning workloads, ideal for model training and fine-tuning with popular frameworks.",
    "is_new": true,
    "title": "Serverless GPU compute"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_extract",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "Extract entities from text using AI with `ai_extract()` function in Databricks SQL and Runtime, supporting English and multiple languages.",
    "is_new": true,
    "title": "ai_extract function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/udf/python-udtf",
    "type": "Public Preview",
    "description": "A user-defined table function (UDTF) allows you to register functions that return tables instead of scalar values. Unlike scalar functions that return a single result value from each call, each UDTF is invoked in a SQL statement'sFROMclause and returns an entire table as output.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-05-30",
    "ai_summary": "User-defined table functions (UDTFs) in Databricks allow registering Python classes that return tables instead of scalar values.",
    "is_new": true,
    "title": "Python user-defined table functions (UDTFs)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/query/postgres-compatibility",
    "type": "Public Preview",
    "description": "This page describes how a Lakebase database instance is compatible with Postgres. As a managed Postgres service, there are some differences and functionality limitations.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Postgres compatibility for Lakebase database instances, with limitations and differences from managed Postgres services.",
    "is_new": true,
    "title": "Postgres compatibility"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/dlt-multi-file-editor",
    "type": "Beta",
    "description": "This article describes using the Lakeflow Pipelines Editor to develop and debug ETL (extract, transform, and load) pipelines inLakeflow Declarative Pipelines.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-25",
    "ai_summary": "Develop and debug ETL pipelines in Lakeflow Declarative Pipelines using the Lakeflow Pipelines Editor, an IDE with code-first workflows and",
    "is_new": true,
    "title": "Develop and debug ETL pipelines with the Lakeflow Pipelines Editor"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta-sharing/create-share",
    "type": "Public Preview",
    "description": "This page explains how to create and manage shares forDelta Sharing.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Create and manage shares for Delta Sharing, securing data assets in Unity Catalog for sharing with recipients.",
    "is_new": true,
    "title": "Create and manage shares for Delta Sharing"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-streaming-table",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQL",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-10",
    "ai_summary": "Create a streaming table with incremental data processing, refreshing only new data and ignoring updates during execution.",
    "is_new": true,
    "title": "CREATE STREAMING TABLE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/evaluate-prompts",
    "type": "Beta",
    "description": "",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-18",
    "ai_summary": "Evaluate and track machine learning prompts across app versions, deploy top performers, and learn evaluation fundamentals in MLflow.",
    "is_new": true,
    "title": "Evaluate prompts"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-materialized-view",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQL",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-10",
    "ai_summary": "Materialized views: precompute query results, update on changes, backed by ETL pipeline; create and refresh using Pro or Serverless SQL",
    "is_new": true,
    "title": "CREATE MATERIALIZED VIEW"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/collate",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.1 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Attaches an explicit collation to an ASTRING expression in Databricks SQL and Runtime 16.1+ using the COLLATION_TABLE function.",
    "is_new": true,
    "title": "collate expression"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/",
    "type": "Public Preview",
    "description": "This page introducesDatabricksLakebase, a fully managed Postgres OLTP database engine, integrated into the Databricks Data Intelligence Platform. A database instance is a type ofDatabrickscompute that provides the storage and compute for running a Postgres server that manages multiple databases.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Databricks Lakebase introduces a fully managed Postgres OLTP database engine for integrating transactional workloads with data lakes and warehouses.",
    "is_new": true,
    "title": "What is Lakebase?"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/mcp/",
    "type": "Beta",
    "description": "This page explains how to useMCPon Databricks. MCP is an open source standard that connects AI agents to tools, resources, prompts, and other contextual information.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Databricks enables standardization and connectivity between AI agents, tools, and resources through Managed and Custom Multi-Cloud PaaS options.",
    "is_new": true,
    "title": "Model context protocol (MCP) on Databricks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/if-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-12",
    "ai_summary": "Executes a list of statements based on the first condition that evaluates to true in a compound statement.",
    "is_new": true,
    "title": "IF THEN ELSE statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/query-federation/http",
    "type": "Public Preview",
    "description": "This article describes how to set up Lakehouse Federation to run federated queries onexternal servicedata that is not managed byDatabricks. To learn more about Lakehouse Federation, seeWhat is Lakehouse Federation?.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-02",
    "ai_summary": "Set up Lakehouse Federation to run federated queries on external data, requiring connection setup, authentication methods, and specific workspace and",
    "is_new": true,
    "title": "Connect to external HTTP services"
  },
  {
    "url": "https://docs.databricks.com/aws/en/libraries/notebooks-python-libraries",
    "type": "Public Preview",
    "description": "Notebook-scoped libraries let you create, modify, save, reuse, and share custom Python environments that are specific to a notebook. When you install a notebook-scoped library, only the current notebook and any jobs associated with that notebook have access to that library. Other notebooks attached to the same cluster are not affected.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-17",
    "ai_summary": "Notebook-scoped libraries install custom Python environments for individual notebooks, not shared across sessions or clusters.",
    "is_new": true,
    "title": "Notebook-scoped Python libraries"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/tag-policies/manage-tag-policies",
    "type": "Beta",
    "description": "This page explains how to create and manage tag policies across your account. For an overview of tag policies, seeTag policies.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-09",
    "ai_summary": "Manage tag policies across your account, enabling governance and restrictions on tags, with options to create, edit, or delete.",
    "is_new": true,
    "title": "Create and manage tag policies"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/ai-functions-example",
    "type": "Public Preview",
    "description": "This article illustrates how to use AI Functions to examine customer reviews and determine if a response needs to be generated. The AI Functions used in this example are built-inDatabricks SQLfunctions, powered by generative AI models made available by Databricks Foundation Model APIs. SeeApply AI on data usingDatabricksAI Functions.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-28",
    "ai_summary": "Databricks' AI Functions enable analyzing sentiment, classifying reviews, extracting information, and generating responses for customer feedback using generative AI models.",
    "is_new": true,
    "title": "Analyze customer reviews using AI Functions"
  },
  {
    "url": "https://docs.databricks.com/aws/en/data-governance/unity-catalog/abac/tutorial",
    "type": "Beta",
    "description": "This tutorial introduces how to configure row filter and column mask attribute-based access control (ABAC) policies inUnity Catalog.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-10",
    "ai_summary": "Configure row filter and column mask policies for attribute-based access control (ABAC) to restrict data access in Unity Catalog.",
    "is_new": true,
    "title": "Tutorial: Configure ABAC"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring-agent-framework",
    "type": "Beta",
    "description": "This page describes usage of Agent Evaluation version0.22with MLflow 2. Databricks recommends using MLflow 3, which is integrated with Agent Evaluation>1.0. In MLflow 3, Agent Evaluation APIs are now part of themlflowpackage.For information on this topic, seeProduction quality monitoring (running scorers automatically).",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-03",
    "ai_summary": "Monitor generative AI apps with Mosaic AI Agent Evaluation, tracking operational and quality metrics, including correctness and guideline adherence.",
    "is_new": true,
    "title": "Monitor apps deployed using Agent Framework (MLflow 2)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/leave-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Terminates loop iteration and exits, using `LEAVE` statement within compound statements with specified label.",
    "is_new": true,
    "title": "LEAVE statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/usage/budget-policies",
    "type": "Public Preview",
    "description": "This article explains how to use serverless budget policies to enforce cost attribution tags on serverless compute workloads.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-07",
    "ai_summary": "Create serverless budget policies with cost attribution tags for compute workloads, managing permissions and assigning to users or workspaces.",
    "is_new": true,
    "title": "Attribute usage with serverless budget policies"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/use-prompts-in-deployed-apps",
    "type": "Beta",
    "description": "This guide shows you how to use prompts from the MLflow Prompt Registry in your production GenAI applications.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-24",
    "ai_summary": "Use MLflow Prompt Registry to manage and update prompts in GenAI applications, enabling dynamic updates without redeployment.",
    "is_new": true,
    "title": "Use prompts in deployed applications"
  },
  {
    "url": "https://docs.databricks.com/aws/en/release-notes/product/2025/march",
    "type": "Public Preview",
    "description": "These features andDatabricksplatform improvements were released in March 2025.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-31",
    "ai_summary": "Databricks releases new features and improvements, including Runtime 16.3, MLflow, Mosaic AI, and managed connectors, with various public previews.",
    "is_new": true,
    "title": "March 2025"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/create-fine-tune-run",
    "type": "Public Preview",
    "description": "This article describes how to create and configure a training run using theFoundation Model Fine-tuning(now part of Mosaic AI Model Training) API, and describes all of the parameters used in the API call. You can also create a run using the UI. For instructions, seeCreate a training run using theFoundation Model Fine-tuningUI.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-26",
    "ai_summary": "Create and configure a Foundation Model Fine-tuning API run with parameters for model training, evaluation, and registration in Unity Catalog.",
    "is_new": true,
    "title": "Create a training run using the Foundation Model Fine-tuning API"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta-sharing/read-data-databricks",
    "type": "Public Preview",
    "description": "This page describes how to read data shared with you using theDatabricks-to-DatabricksDelta Sharingprotocol, where Databricks manages a secure connection for data sharing. Unlike theDelta Sharingopen sharingprotocol, the Databricks-to-Databricks protocol does not require a credential file (token-based security).",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Read shared data using Databricks-to-Databricks Delta Sharing protocol; requires Unity Catalog-enabled workspace and token-free security.",
    "is_new": true,
    "title": "Read data shared using Databricks-to-Databricks Delta Sharing (for recipients)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/legacy/uniform",
    "type": "Public Preview",
    "description": "This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. SeeRead Delta tables with Iceberg clients.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-06-12",
    "ai_summary": "Enable Delta UniForm to read Delta tables with Apache Iceberg clients using a single data file and metadata layer.",
    "is_new": true,
    "title": "Legacy UniForm IcebergCompatV1"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-scripting",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Script SQL with procedural logic, using standard syntax for declaring variables, handling exceptions, and controlling flow.",
    "is_new": true,
    "title": "SQL scripting"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dashboards/datasets/custom-calculations",
    "type": "Public Preview",
    "description": "Custom calculations let you define dynamic metrics and transformations without modifying dataset queries. This article explains how to use custom calculations in AI/BI dashboards.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Define dynamic metrics and transformations in AI/BI dashboards with custom calculations, creating new fields without modifying SQL queries.",
    "is_new": true,
    "title": "What are custom calculations?"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/account-settings/budgets",
    "type": "Public Preview",
    "description": "This article explains how to use budgets to track spending in yourDatabricksaccount.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Databricks introduces budgets to track spending across accounts, teams, projects, or workspaces, measured in USD and enabling usage monitoring.",
    "is_new": true,
    "title": "Create and monitor budgets"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/query/sql-editor",
    "type": "Public Preview",
    "description": "This page describes how to access a Lakebase database instance from the SQL editor to run PostgreSQL commands and queries.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Access Lakebase database instances from SQL editor, verifying Postgres role and permissions, to run PostgreSQL commands and queries.",
    "is_new": true,
    "title": "Access a database instance from the SQL editor"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/admin-guide/billable-usage-delivery",
    "type": "Public Preview",
    "description": "Billable usage logs do not record usage for all products. Databricks recommends usingsystem tablesto view complete usage logs.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Configure daily delivery of billable usage logs to an AWS S3 bucket, viewable by account admins and owners.",
    "is_new": true,
    "title": "Deliver and access billable usage logs (legacy)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/read_pulsar",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime14.1 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-01-23",
    "ai_summary": "Pulsar table-valued function for streaming queries, requires named parameters and mandatory options like service URL and topic.",
    "is_new": true,
    "title": "read_pulsar streaming table-valued function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/genie/file-upload",
    "type": "Public Preview",
    "description": "This article explains how to upload CSV and Excel files directly into a Genie space for analysis using natural language and in combination with other tables in the space. To enable file uploads, contact your Databricks account team.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-03",
    "ai_summary": "Upload CSV/Excel files to Genie spaces for analysis, enabling quick data validation and exploration with Unity Catalog tables.",
    "is_new": true,
    "title": "Upload a file"
  },
  {
    "url": "https://docs.databricks.com/aws/en/partners/ingestion/infoworks",
    "type": "Public Preview",
    "description": "InfoworksDataFoundry is an automated enterprise data operations and orchestration system that runs natively onDatabricksand leverages the full power ofDatabricksto deliver a easy solution for data onboarding\u2014an important first step in operationalizing your data lake.\nDataFoundry not only automates data ingestion, but also automates the key functionality that must accompany ingestion to establish a foundation for analytics.\nData onboarding with DataFoundry automates:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2023-10-10",
    "ai_summary": "Infoworks DataFoundry automates data onboarding, ingestion, and governance on Databricks, using personal access tokens and instance profiles for secure access",
    "is_new": true,
    "title": "Connect to Infoworks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/monitoring",
    "type": "Beta",
    "description": "This page describes usage of Agent Evaluation version0.22with MLflow 2. Databricks recommends using MLflow 3, which is integrated with Agent Evaluation>1.0. In MLflow 3, Agent Evaluation APIs are now part of themlflowpackage.For information on this topic, seeProduction quality monitoring (running scorers automatically).",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-25",
    "ai_summary": "Monitor generative AI apps with Lakehouse Monitoring for GenAI, integrating MLflow Tracing and Agent Evaluation for quality and performance tracking.",
    "is_new": true,
    "title": "What is Lakehouse Monitoring for generative AI? (MLflow 2)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/pg-roles",
    "type": "Public Preview",
    "description": "A Postgres role for the Lakebase database instance owner\u2019sDatabricksidentity is created automatically. Initially, only the owner of the instance can log in and access the instance through Postgres. To allow otherDatabricksidentities to log in to the database instance, the owner needs to create corresponding Postgres roles.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Create Postgres roles based on Databricks identities using the `databricks_create_role` function, with options for users, service principals, and groups.",
    "is_new": true,
    "title": "Create and manage Postgres roles for Databricks identities"
  },
  {
    "url": "https://docs.databricks.com/aws/en/compute/gpu",
    "type": "Public Preview",
    "description": "Some GPU-enabled instance types are inBetaand are marked as such in the drop-down list when you select the driver and worker types during compute creation.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-14",
    "ai_summary": "Databricks supports GPU-enabled compute instances for accelerated processing, with NVIDIA H100, A100, and L40S GPUs available.",
    "is_new": true,
    "title": "GPU-enabled compute"
  },
  {
    "url": "https://docs.databricks.com/aws/en/data-governance/unity-catalog/external-lineage",
    "type": "Public Preview",
    "description": "This page describes how to update data lineage to include external assets and workflows that are run outside ofDatabricks.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-29",
    "ai_summary": "Add external data lineage metadata to Unity Catalog for end-to-end views, capturing sources and consumers outside Databricks.",
    "is_new": true,
    "title": "Bring your own data lineage"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-collation",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.1 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Databricks introduces collations for string comparisons, allowing for case-insensitive, accent-insensitive, and language-aware sorting and matching.",
    "is_new": true,
    "title": "Collation"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/fine-tune-run-tutorial",
    "type": "Public Preview",
    "description": "This article describes how to create and configure a run using theFoundation Model Fine-tuning(now part of Mosaic AI Model Training) API, and then review the results and deploy the model using the Databricks UI and Mosaic AI Model Serving.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-14",
    "ai_summary": "Create and configure fine-tuning runs for Mosaic AI models, review results, and deploy using Databricks UI and Mosaic AI Model",
    "is_new": true,
    "title": "Tutorial: Create and deploy a Foundation Model Fine-tuning run"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/for-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Databricks FOR statement executes SQL statements for each row returned by query, with optional label and variable name.",
    "is_new": true,
    "title": "FOR statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/runtime-release-notes/5.1",
    "type": "Public Preview",
    "description": "Support for thisDatabricks Runtimeversion has ended. For the end-of-support date, seeEnd-of-support history. For all supportedDatabricks Runtimeversions, seeDatabricks Runtimerelease notes versions and compatibility.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-03",
    "ai_summary": "Databricks Runtime 5.1 deprecated; features include Delta Lake support, tunable isolation levels, and in-place Parquet-to-Delta conversion.",
    "is_new": true,
    "title": "Databricks Runtime 5.1 (EoS)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/loop-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Repeat a list of statements in Databricks SQL with optional label and loop control using LEAVE or ITERATE.",
    "is_new": true,
    "title": "LOOP statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/routines",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Information Schema Routines feature lists functions and procedures in Unity Catalog, with varying columns providing details.",
    "is_new": true,
    "title": "ROUTINES"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/machine-learning/ai-generate-text-example",
    "type": "Public Preview",
    "description": "The AI function,ai_generate_text()is deprecated. Databricks recommends usingai_query with external models.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-05",
    "ai_summary": "Use Databricks SQL function ai_generate_text() to analyze customer reviews, determine response needs, and generate responses in public preview.",
    "is_new": true,
    "title": "Analyze customer reviews with ai_generate_text() and OpenAI"
  },
  {
    "url": "https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/add-data-external-locations",
    "type": "Public Preview",
    "description": "This article describes how to use the add data UI to create a managed table from data inAmazon S3using aUnity Catalogexternal location. An external location is an object that combines a cloud storage path with a storage credential that authorizes access to the cloud storage path.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-11-07",
    "ai_summary": "Create managed tables from Amazon S3 data using Unity Catalog external locations in Databricks.",
    "is_new": true,
    "title": "Load data using a Unity Catalog external location"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-bricks/knowledge-assistant",
    "type": "Beta",
    "description": "This page describes how to useAgent Bricks: Knowledge Assistantto create a question-and-answer chatbot over your documents and improve its quality based on natural language feedback from your subject matter experts.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-24",
    "ai_summary": "Create a chatbot using Agent Bricks: Knowledge Assistant, a no-code AI tool, to answer questions with citations and improve performance.",
    "is_new": true,
    "title": "Use Agent Bricks: Knowledge Assistant to create a high-quality chatbot over your documents"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dev-tools/databricks-connect/cluster-config",
    "type": "Public Preview",
    "description": "This article coversDatabricks Connect forDatabricks Runtime13.3 LTS and above.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-13",
    "ai_summary": "Configure Databricks Connect to connect IDEs like Visual Studio Code to Databricks clusters or serverless compute with Unity Catalog enabled.",
    "is_new": true,
    "title": "Compute configuration for Databricks Connect"
  },
  {
    "url": "https://docs.databricks.com/aws/en/metric-views/",
    "type": "Public Preview",
    "description": "Metric views provide a centralized way to define and manage consistent, reusable, and governed core business metrics. This page explains metric views, how to define them, control access, and query them in downstream tools.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-29",
    "ai_summary": "Metric views centralize business metrics, defining complex logic once for reuse across reporting tools with controlled access and querying.",
    "is_new": true,
    "title": "Unity Catalog metric views"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/workspace-level-scim/aad",
    "type": "Public Preview",
    "description": "This documentation has been retired and might not be updated. Workspace-level SCIM provisioning is legacy. Databricks recommends that you use account-level SCIM provisioning, seeSync users and groups fromyour identity providerusing SCIM.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-06",
    "ai_summary": "Configure SCIM provisioning in Databricks workspaces using Microsoft Entra ID, requiring Premium plan and Cloud Application Administrator role.",
    "is_new": true,
    "title": "Configure workspace-level SCIM provisioning using Microsoft Entra ID (legacy)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/compound-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Compound SQL statements allow for multiple queries, control flow, and variables in a single block.",
    "is_new": true,
    "title": "BEGIN END compound statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/ingestion/data-migration/clone-parquet",
    "type": "Public Preview",
    "description": "You can useDatabricksclone functionality to incrementally convert data from Parquet orApache Icebergdata sources to managed or external Delta tables.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-24",
    "ai_summary": "Databricks' clone feature incrementally converts Parquet or Apache Iceberg data to Delta tables, ideal for migration, syncing, and reporting.",
    "is_new": true,
    "title": "Incrementally clone Parquet and Apache Iceberg tables to Delta Lake"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dev-tools/ci-cd/github",
    "type": "Public Preview",
    "description": "GitHub Actionscan be used to trigger runs of your CI/CD workflows from your GitHub repositories and allows you to automate your build, test, and deployment CI/CD pipeline.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-18",
    "ai_summary": "Databricks integrates GitHub Actions for CI/CD workflows, enabling automated build, test, and deployment processes from GitHub repositories.",
    "is_new": true,
    "title": "GitHub Actions"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/repeat-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Databricks introduces REPEAT statement in Public Preview, repeating SQL statements until condition is true within compound statements.",
    "is_new": true,
    "title": "REPEAT statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-qry-select-cte",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Databricks SQL: Common Table Expressions (CTEs) allow referencing temporary results sets in SELECT statements with recursive queries.",
    "is_new": true,
    "title": "Common table expression (CTE)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/partners/ingestion/streamsets",
    "type": "Public Preview",
    "description": "StreamSetshelps you to manage and monitor your data flow throughout its lifecycle.StreamSetsnative integration withDatabricksandDelta Lakeallows you to pull data from various sources and manage your pipelines easily.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2023-10-10",
    "ai_summary": "StreamSets integrates with Databricks, allowing data flow management and monitoring, with steps for setup and authentication.",
    "is_new": true,
    "title": "Connect to StreamSets"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-framework/external-connection-tools",
    "type": "Public Preview",
    "description": "Learn how to connectAI agent toolsto external applications like Slack, Google Calendar, or any service with an API using HTTP requests. Agents can use externally connected tools to automate tasks, send messages, and retrieve data from third-party platforms.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-14",
    "ai_summary": "Connect AI agents to external services like Slack or Google Calendar using HTTP requests with Unity Catalog and Python.",
    "is_new": true,
    "title": "Connect AI agent tools to external services"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/migrate-to-dpm",
    "type": "Public Preview",
    "description": "This article describes how to migrate pipelines that use theLIVEvirtual schema (the legacy publishing mode) to the default publishing mode.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Migrate pipelines from legacy publishing mode to default publishing mode with simplified syntax and support for multiple catalogs and schemas.",
    "is_new": true,
    "title": "Enable the default publishing mode in a pipeline"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_similarity",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "The ai_similarity() function compares two strings, computing semantic similarity scores using Foundation Model APIs in public preview.",
    "is_new": true,
    "title": "ai_similarity function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/compute/serverless/dependencies",
    "type": "Public Preview",
    "description": "This article explains how to use a serverless notebook'sEnvironmentside panel to configure dependencies, serverless budget policies, memory, and environment version. This panel provides a single place to manage the notebook's serverless settings. Settings configured in this panel only apply when the notebook is connected to serverless compute.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-05",
    "ai_summary": "Configure serverless notebook settings, including dependencies, memory, and budget policies, in the Environmentside panel for optimized performance.",
    "is_new": true,
    "title": "Configure the serverless environment"
  },
  {
    "url": "https://docs.databricks.com/aws/en/release-notes/dlt/2024/49/",
    "type": "Private Preview",
    "description": "December 9 - 12, 2024",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-23",
    "ai_summary": "Databricks releases version 2024.49 with single node compute for Unity Catalog pipelines and improved materialized view/streaming table deletion timing.",
    "is_new": true,
    "title": "DLT release 2024.49"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/from_xml",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime14.1 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-05-27",
    "ai_summary": "Parses XML records into struct or variant values using a schema definition, with optional directives for handling corrupt records.",
    "is_new": true,
    "title": "from_xml function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/jobs/repair-job-failures",
    "type": "Public Preview",
    "description": "Suppose you have been notified (for example, through an email notification, a monitoring solution, or in the Lakeflow Jobs UI) that a task has failed in a run of your job. The steps in this article provide guidance to help you identify the cause of failure, suggestions to fix the issues that you find, and how to repair failed job runs.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Troubleshoot failed job runs by identifying cause, fixing issues, and repairing tasks using Lakeflow Jobs UI and task run details.",
    "is_new": true,
    "title": "Troubleshoot and repair job failures"
  },
  {
    "url": "https://docs.databricks.com/aws/en/query-federation/teradata",
    "type": "Public Preview",
    "description": "This article describes how to set up Lakehouse Federation to run federated queries onTeradatadata that is not managed byDatabricks. To learn more about Lakehouse Federation, seeWhat is Lakehouse Federation?.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-31",
    "ai_summary": "Set up Lakehouse Federation to run federated queries on Teradata data without Databricks management, following specific requirements and permissions.",
    "is_new": true,
    "title": "Run federated queries on Teradata"
  },
  {
    "url": "https://docs.databricks.com/aws/en/search/",
    "type": "Public Preview",
    "description": "This article describes how to search for tables, volumes, notebooks, queries, dashboards, alerts, files, folders, libraries, jobs, repos, partners, and Marketplace listings in yourDatabricksworkspace.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-23",
    "ai_summary": "Search for tables, notebooks, queries, and more in your Databricks workspace using intelligent or navigational search with filtering options.",
    "is_new": true,
    "title": "Search for workspace objects"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dev-tools/databricks-utils",
    "type": "Public Preview",
    "description": "This article contains reference for Databricks Utilities (dbutils). The utilities provide commands that enable you to work with your Databricks environment from notebooks. For example, you can manage files and object storage, and work with secrets.dbutilsare available in Python, R, and Scala notebooks.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-07",
    "ai_summary": "Databricks Utilities provide commands to manage files, object storage, secrets, and more in Python, R, and Scala notebooks.",
    "is_new": true,
    "title": "Databricks Utilities (dbutils) reference"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/constraint_column_usage",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-18",
    "ai_summary": "INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE lists constraints referencing columns as foreign or primary keys in Unity Catalog and Databricks SQL.",
    "is_new": true,
    "title": "CONSTRAINT_COLUMN_USAGE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dev-tools/sdk-java",
    "type": "Beta",
    "description": "Databricks recommendsDatabricks Asset Bundlesfor creating, developing, deploying, and testing jobs and other Databricks resources as source code. SeeWhat areDatabricks Asset Bundles?.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-10-10",
    "ai_summary": "Learn to automate Databricks operations and accelerate development with the Databricks SDK for Java, including setup and dependencies.",
    "is_new": true,
    "title": "Databricks SDK for Java"
  },
  {
    "url": "https://docs.databricks.com/aws/en/integrations/google-sheets",
    "type": "Public Preview",
    "description": "This page describes how to use theDatabricksConnector for Google Sheetsto connect toDatabricksfrom Google Sheets. The Databricks add-on enables you to query Databricks data from within Google Sheets, allowing you to conduct further analysis.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-01",
    "ai_summary": "Connect Databricks to Google Sheets using the Databricks Connector, enabling data analysis and querying within Google Sheets.",
    "is_new": true,
    "title": "Connect to Databricks from Google Sheets"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/model-serving/inference-tables",
    "type": "Public Preview",
    "description": "This article describes the legacy inference table experience which is only relevant for certain provisioned throughput and custom model endpoints. This experience is not recommended. Databricks recommendsAI Gateway-enabled inference tablesfor its availability on custom model, foundation model and agent serving endpoints.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-14",
    "ai_summary": "Inference tables log model serving endpoint requests and responses, enabling monitoring, debugging, and optimization of ML models in production workflows.",
    "is_new": true,
    "title": "Inference tables for monitoring and debugging models"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/feature-store/online-feature-store",
    "type": "Beta",
    "description": "us-east-1,us-west-2,eu-west-1,ap-southeast-1,ap-southeast-2,eu-central-1,us-east-2,ap-south-1",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Databricks Online Feature Stores provide low-latency access to feature data for real-time applications and machine learning models, with scalable capacity",
    "is_new": true,
    "title": "Databricks Online Feature Stores"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/create/",
    "type": "Public Preview",
    "description": "To use Lakebase, you must first create a database instance. This page explains how to create and manage a database instance.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Create and manage Lakebase database instances in Public Preview regions, enabling Postgres OLTP databases for managed instances.",
    "is_new": true,
    "title": "Create and manage a database instance"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/ai-functions",
    "type": "Public Preview",
    "description": "This article describesDatabricksAI Functions and the supported functions.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-24",
    "ai_summary": "Databricks introduces AI Functions for applying text analysis, translation, and more to data, with general-purpose and task-specific functions available.",
    "is_new": true,
    "title": "Apply AI on data using Databricks AI Functions"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/dlt-notebook-devex",
    "type": "Public Preview",
    "description": "This article describes how to use a notebook inLakeflow Declarative Pipelinesto develop and debug ETL pipelines. This is the default development experience inLakeflow Declarative Pipelines.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-21",
    "ai_summary": "Use notebooks in Lakeflow Declarative Pipelines to develop and debug ETL pipelines with features like validation and cluster status viewing.",
    "is_new": true,
    "title": "Develop and debug ETL pipelines with a notebook in Lakeflow Declarative Pipelines"
  },
  {
    "url": "https://docs.databricks.com/aws/en/connect/streaming/kafka",
    "type": "Public Preview",
    "description": "This article describes how you can use Apache Kafka as either a source or a sink when runningStructured Streamingworkloads onDatabricks.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-14",
    "ai_summary": "Databricks uses Apache Kafka as source or sink for Structured Streaming workloads, providing options for subscription and configuration.",
    "is_new": true,
    "title": "Stream processing with Apache Kafka and Databricks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_classify",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "AI Function: ai_classify() classifies input text into provided labels using Foundation Model APIs in supported regions.",
    "is_new": true,
    "title": "ai_classify function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/semi-structured/variant",
    "type": "Public Preview",
    "description": "This article describes the Databricks SQL operators you can use to query and transform semi-structured data stored asVARIANT. TheVARIANTdata type is available inDatabricks Runtime15.3 and above.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-03",
    "ai_summary": "Databricks SQL operators for querying semi-structured data stored as VARIANT, including extraction, nesting, and array indexing.",
    "is_new": true,
    "title": "Query variant data"
  },
  {
    "url": "https://docs.databricks.com/aws/en/semi-structured/variant-json-diff",
    "type": "Public Preview",
    "description": "This article describes the behavior changes and differences in syntax and semantics when working with the variant data type. This article assumes that you are familiar with working with JSON string data onDatabricks. For users new toDatabricks, you should use variant over JSON strings whenever storing semi-structured data that requires flexibility for changing or unknown schema. SeeModel semi-structured data.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-03",
    "ai_summary": "Databricks' Variant data type replaces JSON strings for semi-structured data, offering improved performance and query capabilities.",
    "is_new": true,
    "title": "How is variant different than JSON strings?"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/key_column_usage",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-18",
    "ai_summary": "Databricks SQL and Unity Catalog: INFORMATION_SCHEMA.KEY_COLUMN_USAGE relation lists primary/foreign key constraints with column details.",
    "is_new": true,
    "title": "KEY_COLUMN_USAGE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/query-federation/salesforce-data-cloud-file-sharing",
    "type": "Public Preview",
    "description": "This page describes how to read data in Salesforce Data Cloud using the file sharing connector.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-04",
    "ai_summary": "Databricks offers three Salesforce connectors: file sharing, query federation, and ingestion, each with unique use cases and features.",
    "is_new": true,
    "title": "Lakehouse Federation for Salesforce Data Cloud File Sharing"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-alter-share",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime10.4 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-12",
    "ai_summary": "Alter a Unity Catalog share by adding/removing schema/table/view, renaming, or transferring ownership; requires specific permissions and privileges.",
    "is_new": true,
    "title": "ALTER SHARE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/workspace-level-scim/okta",
    "type": "Public Preview",
    "description": "This documentation has been retired and might not be updated. Workspace-level SCIM provisioning is legacy. Databricks recommends that you use account-level SCIM provisioning, seeSync users and groups fromyour identity providerusing SCIM.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-06",
    "ai_summary": "Legacy workspace-level SCIM provisioning retired; use account-level SCIM provisioning instead, configuring Okta with Databricks API token and URLs.",
    "is_new": true,
    "title": "Configure workspace-level SCIM provisioning using Okta (legacy)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/create-and-edit-prompts",
    "type": "Beta",
    "description": "This guide shows you how to create new prompts and manage their versions in the MLflow Prompt Registry using the MLflow Python SDK. All of the code on this page is included in theexample notebook.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-20",
    "ai_summary": "Create, manage, and use MLflow prompts programmatically with Python SDK; includes prerequisites, steps, and examples for prompt registry usage.",
    "is_new": true,
    "title": "Create and edit prompts"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/train-model/serverless-forecasting",
    "type": "Public Preview",
    "description": "This article shows you how to run a serverless forecasting experiment using the Mosaic AI Model Training UI.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-16",
    "ai_summary": "Run serverless forecasting experiments using Mosaic AI Model Training UI with Unity Catalog tables and specify time column, frequency, and",
    "is_new": true,
    "title": "Forecasting (serverless) with AutoML"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/oauth",
    "type": "Public Preview",
    "description": "This page describes how to obtain an OAuth token from the Lakebase database instance and use it to authenticate to the database instance. An OAuth token is needed if you are connecting to your database from psql or a notebook.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-11",
    "ai_summary": "Obtain an OAuth token for PostgreSQL authentication in Databricks, ensuring SSL connections and workspace-scoped tokens.",
    "is_new": true,
    "title": "Authenticate to database instance"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-procedure",
    "type": "Public Preview",
    "description": "Applies to::Databricks Runtime17.0 and aboveUnity Catalogonly",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Databricks Unity Catalog allows creating procedures with parameters, replacing or checking for existence, and executing SQL statements.",
    "is_new": true,
    "title": "CREATE PROCEDURE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-alter-table-manage-column",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-14",
    "ai_summary": "ALTER TABLE statement adds, modifies, or drops columns or fields in Delta Lake tables with various options.",
    "is_new": true,
    "title": "ALTER TABLE ... COLUMN clause"
  },
  {
    "url": "https://docs.databricks.com/aws/en/workspace/workspace-assets",
    "type": "Public Preview",
    "description": "This article provides a high-level introduction toDatabricksworkspace objects. You can create, view, and organize workspace objects in the workspace browser across personas.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-11",
    "ai_summary": "Databricks workspace objects: create, view, and organize notebooks, clusters, jobs, libraries, and data sources across personas and workspaces.",
    "is_new": true,
    "title": "Introduction to workspace objects"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_summarize",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "Databricks AI Function \"ai_summarize\" generates text summaries using SQL, available in Public Preview with English language tuning.",
    "is_new": true,
    "title": "ai_summarize function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/data-types/variant-type",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime15.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Databricks' VARIANT type represents semi-structured data, supports JSON parsing and casting, with methods for extraction and error handling.",
    "is_new": true,
    "title": "VARIANT type"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-aux-describe-procedure",
    "type": "Public Preview",
    "description": "Applies to:Databricks Runtime17.0 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Returns basic metadata information about an existing procedure, including name and parameters, with optional extended usage info.",
    "is_new": true,
    "title": "DESCRIBE PROCEDURE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/collations",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.1 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Returns supported collations in Databricks, including language, country, name, and sensitivity information for Unicode and ICU versions.",
    "is_new": true,
    "title": "collations table function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/metric-views/yaml-ref",
    "type": "Public Preview",
    "description": "This page describes each component of the YAML used to define a metric view.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "Databricks metric view YAML definition has six top-level fields: version, source, joins, filter, dimensions, and measures.",
    "is_new": true,
    "title": "Metric view YAML reference"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/production-monitoring",
    "type": "Beta",
    "description": "Production monitoring enables continuous quality assessment of your GenAI applications by automatically running scorers on live traffic. The monitoring service runs every 15 minutes, evaluating a configurable sample of traces using the same scorers you use in development.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-18",
    "ai_summary": "Production monitoring in beta enables automated quality assessment of GenAI applications by running scorers on live traffic and tracking traces.",
    "is_new": true,
    "title": "Production Monitoring"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/iterate-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "Terminates iteration of looping statement and continues with next iteration if condition met, only within compound statements.",
    "is_new": true,
    "title": "ITERATE statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/data-governance/unity-catalog/abac/",
    "type": "Beta",
    "description": "This page describes attribute-based access control (ABAC) inUnity Catalog.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-10",
    "ai_summary": "Databricks' ABAC enables flexible, scalable access control via governed tags, policies, and UDFs for unified data governance and security.",
    "is_new": true,
    "title": "Unity Catalog attribute-based access control (ABAC)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/usage/",
    "type": "Public Preview",
    "description": "The articles in this section outline the available cost controls and cost monitoring features available on Databricks.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Databricks provides cost controls, monitoring features, and tagging options for accurate attribution and budgeting.",
    "is_new": true,
    "title": "Cost management tools on Databricks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/workspace-level-scim/",
    "type": "Public Preview",
    "description": "This documentation has been retired and might not be updated. Workspace-level SCIM provisioning is legacy. Databricks recommends that you use account-level SCIM provisioning, seeSync users and groups fromyour identity providerusing SCIM.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-06",
    "ai_summary": "Retired doc: Legacy workspace-level SCIM provisioning; use account-level SCIM for identity federation and user/group management.",
    "is_new": true,
    "title": "Provision identities to a Databricks workspace (legacy)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/partners/ingestion/stitch",
    "type": "Public Preview",
    "description": "Stitchhelps you consolidate all your business data from different databases and SaaS applications (Salesforce, Hubspot, Marketo, and so on) intoDelta Lake.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2023-10-10",
    "ai_summary": "Stitch consolidates business data from various sources into Delta Lake; integrates with Databricks using personal access tokens and instance profiles.",
    "is_new": true,
    "title": "Connect to Stitch"
  },
  {
    "url": "https://docs.databricks.com/aws/en/compute/single-user-fgac",
    "type": "Public Preview",
    "description": "Fine-grained access control allows you to restrict access to specific data using views, row filters, and column masks. This page explains how serverless compute is used to enforce fine-grained access controls on dedicated compute resources.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-20",
    "ai_summary": "Databricks' fine-grained access control allows restricting data access through views, filters, and masks, enabled by serverless compute on dedicated resources.",
    "is_new": true,
    "title": "Fine-grained access control on dedicated compute"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/account-settings/usage-detail-tags",
    "type": "Public Preview",
    "description": "This article explains how to use tags to attribute compute usage to specific workspaces, teams, projects, or users to support cost tracking and budgeting.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-18",
    "ai_summary": "Databricks uses default and custom tags to attribute compute usage to workspaces, teams, projects, or users for cost tracking.",
    "is_new": true,
    "title": "Use tags to attribute and track usage"
  },
  {
    "url": "https://docs.databricks.com/aws/en/data-governance/unity-catalog/manage-privileges/privileges",
    "type": "Public Preview",
    "description": "This page describes theUnity Catalogsecurable objects and the privileges that apply to them. To learn how to grant privileges inUnity Catalog, seeShow, grant, and revoke privileges.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-29",
    "ai_summary": "Unity Catalog securable objects include metastore, catalog, schema, and table, with privileges managed using SQL commands.",
    "is_new": true,
    "title": "Unity Catalog privileges and securable objects"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-framework/structured-retrieval-tools",
    "type": "Public Preview",
    "description": "This article shows how to createAI agent toolsfor structured data retrieval using the Mosaic AI Agent Framework. To allow agents to query structured data sources such as SQL tables, you can use one of the following methods:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-13",
    "ai_summary": "Create AI tools for structured data retrieval using Mosaic AI Agent Framework, with options for fixed or flexible query formats.",
    "is_new": true,
    "title": "Structured retrieval AI agent tools"
  },
  {
    "url": "https://docs.databricks.com/aws/en/database-objects/tags",
    "type": "Beta",
    "description": "This page shows how to apply tags toUnity Catalogsecurable objects.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-09",
    "ai_summary": "Apply tags to Unity Catalog objects for organization and search; govern tags with policies for security and consistency.",
    "is_new": true,
    "title": "Apply tags to Unity Catalog securable objects"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/query/notebook",
    "type": "Public Preview",
    "description": "This page contains code examples that show you how to access your Lakebase database instance throughDatabricksnotebooksand run queries using Python and Scala.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-11",
    "ai_summary": "Access Lakehouse databases from Databricks notebooks using Python or Scala with single connections, connection pools, or M2M OAuth tokens.",
    "is_new": true,
    "title": "Use a notebook to access a database instance"
  },
  {
    "url": "https://docs.databricks.com/aws/en/security/keys/encrypt-otw",
    "type": "Public Preview",
    "description": "Theexample init scriptthat is referenced in this article derives its shared encryption secret from the hash of the keystore stored in DBFS. If you rotate the secret by updating the keystore file in DBFS, all running clusters must be restarted. Otherwise, Spark workers may to fail to authenticate with the Spark driver due to inconsistent shared secret, causing jobs to slow down. Furthermore, since the shared secret is stored in DBFS, any user with DBFS access can retrieve the secret using a notebook.As an alternative, you can use one of the following AWS instance types, which automatically encrypt data between worker nodes with no extra configuration required:General purpose:M-fleet,Md-fleet,M5dn,M5n,M5zn,M7g,M7gd,M6i,M7i,M6id,M6in,M6idnCompute optimized:C-fleet,C5a,C5ad,C5n,C6gn,C7g,C7gd,C7gn,C6i,C6id,C7i,C6inMemory optimized:R-fleet,Rd-fleet,R7g,R7gd,R6i,R7i,R7iz,R6id,R6in,R6idnStorage optimized:D3,D3en,P3dn,R5dn,R5n,I4i,I3enAccelerated computing:G4dn,G5,P4d,P4de,P5",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-01",
    "ai_summary": "Public preview feature: automatic encryption for clusters; requires Enterprise plan; alternatives include specific AWS instance types.",
    "is_new": true,
    "title": "Encrypt traffic between cluster worker nodes"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow/deployment-job",
    "type": "Public Preview",
    "description": "Deployment jobs do not need to be used with MLflow 3 clients or model tracking, and can be enabled on older, existing models in Unity Catalog. However, it is recommended to use MLflow 3.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-10",
    "ai_summary": "Automate ML model deployment and management with deployment jobs, integrating with Unity Catalog and MLflow 3 for seamless workflows.",
    "is_new": true,
    "title": "MLflow 3 deployment jobs"
  },
  {
    "url": "https://docs.databricks.com/aws/en/security/network/serverless-network-security/serverless-firewall",
    "type": "Public Preview",
    "description": "This page describes how to configure a firewall for serverless compute using theDatabricksaccount console UI. You can also use theNetwork Connectivity Configurations API. Firewall enablement is not supported for Amazon S3 or Amazon DynamoDB.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-17",
    "ai_summary": "Configure firewalls for serverless compute using account console or API; requires Premium plan and admin access.",
    "is_new": true,
    "title": "Configure a firewall for serverless compute access"
  },
  {
    "url": "https://docs.databricks.com/aws/en/security/secrets/secrets-spark-conf-env-var",
    "type": "Public Preview",
    "description": "This article provides details about how to reference a secret in a Spark configuration property or environment variable. Retrieved secrets are redacted from notebook output and Spark driver and executor logs.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-14",
    "ai_summary": "Reference secrets in Spark config or env vars; redacted from notebook output and logs, but not from driver logs without",
    "is_new": true,
    "title": "Use a secret in a Spark configuration property or environment variable"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/model-inference/",
    "type": "Public Preview",
    "description": "This article describes what Databricks recommends for batch inference.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-04",
    "ai_summary": "Databricks recommends AI Functions for batch inference, with public preview features for applying AI to stored data.",
    "is_new": true,
    "title": "Deploy models for batch inference and prediction"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/workspace-level-scim/onelogin",
    "type": "Public Preview",
    "description": "This documentation has been retired and might not be updated. Workspace-level SCIM provisioning is legacy. Databricks recommends that you use account-level SCIM provisioning, seeSync users and groups fromyour identity providerusing SCIM.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-06",
    "ai_summary": "Set up SCIM provisioning with OneLogin: generate personal access token, configure app, and enable provisioning for user management.",
    "is_new": true,
    "title": "Configure workspace-level SCIM provisioning using OneLogin (legacy)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/ai-query-batch-inference",
    "type": "Public Preview",
    "description": "This article describes how to perform batch inference usingAI Functionsat scale. The examples in this article are recommended for production scenarios, such as deploying batch inference pipelines as scheduled workflows and usingai_queryand a Databricks-hosted foundation model forStructured Streaming.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-03",
    "ai_summary": "Perform batch inference at scale with AI Functions, including task-specific and general-purpose functions, using Unity Catalog and pre-provisioned models.",
    "is_new": true,
    "title": "Perform batch LLM inference using AI Functions"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/model-serving/manage-serving-endpoints",
    "type": "Public Preview",
    "description": "This article describes how to manage model serving endpoints using theServingUI and REST API. SeeServing endpointsin the REST API reference.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-30",
    "ai_summary": "Manage model serving endpoints using ServingUI and REST API, check status and details programmatically or through MLflow Deployments SDK.",
    "is_new": true,
    "title": "Manage model serving endpoints"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-alter-streaming-table",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQL",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-14",
    "ai_summary": "Databricks SQL allows scheduling refreshes for streaming tables with options for periodic or cron-based schedules.",
    "is_new": true,
    "title": "ALTER STREAMING TABLE"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/track-prompts-app-versions",
    "type": "Beta",
    "description": "This guide shows you how to integrate prompts from the MLflow Prompt Registry into your GenAI applications while tracking both prompt and application versions together. When you usemlflow.set_active_model()with prompts from the registry, MLflow automatically creates lineage between your prompt versions and application versions.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-24",
    "ai_summary": "Integrate MLflow Prompt Registry prompts into GenAI applications, tracking versions and lineage, with automatic updates and logging.",
    "is_new": true,
    "title": "Track prompt versions alongside application versions"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/create/child-instance",
    "type": "Public Preview",
    "description": "This page explains how to create a child instance from an existing Lakebase database instance.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Databricks' Lakebase feature allows creating a copy-on-write child instance from an existing instance at any point in time.",
    "is_new": true,
    "title": "Create a child instance"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-alter-materialized-view",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQL",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-14",
    "ai_summary": "Alters metadata for views, adding or modifying schedules for materialized view refreshes with optional cron syntax.",
    "is_new": true,
    "title": "ALTER MATERIALIZED VIEW"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/roles",
    "type": "Public Preview",
    "description": "This page explains the Postgres roles that you can use to govern access to aDatabricksLakebase database instance, including their privileges, purpose, and configuration.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Databricks Lakebase database instance governance: Postgres roles, privileges, and configuration for administrators, with automatic role creation for instance creators.",
    "is_new": true,
    "title": "Database role types and permissions"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/vector-search-budget-policies",
    "type": "Public Preview",
    "description": "This article describes how to use budget policies to track vector search costs.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-09",
    "ai_summary": "Databricks enables administrators to track vector search costs using budget policies, grouping and filtering billing records across products.",
    "is_new": true,
    "title": "Mosaic AI Vector Search: Budget policies"
  },
  {
    "url": "https://docs.databricks.com/aws/en/compute/sql-warehouse/monitor",
    "type": "Public Preview",
    "description": "You can monitor a SQL warehouse using the Databricks UI.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-29",
    "ai_summary": "Monitor SQL warehouse performance using Databricks UI, tracking status, query count, cluster count, and peak concurrency metrics over various time",
    "is_new": true,
    "title": "Monitor a SQL warehouse"
  },
  {
    "url": "https://docs.databricks.com/aws/en/connect/streaming/pulsar",
    "type": "Public Preview",
    "description": "InDatabricks Runtime14.1 and above, you can useStructured Streamingto stream data from Apache Pulsar onDatabricks.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2023-12-15",
    "ai_summary": "Databricks supports Structured Streaming to read data from Apache Pulsar with exactly-once processing and schema preservation.",
    "is_new": true,
    "title": "Stream from Apache Pulsar"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/instance",
    "type": "Public Preview",
    "description": "A Lakebase database instance manages storage and compute resources and provides the endpoints that users connect to. This page includes an overview that describes a database instance and its general limitations.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Databricks Lakebase manages storage and compute resources, providing endpoints for user connections and offering high availability and scalability.",
    "is_new": true,
    "title": "What is a database instance?"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/unity-catalog",
    "type": "Public Preview",
    "description": "Databricks recommends configuringLakeflow Declarative PipelineswithUnity Catalog.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-14",
    "ai_summary": "Configure Lakeflow pipelines with Unity Catalog for publishing materialized views and streaming tables with fine-grained permissions and compute requirements.",
    "is_new": true,
    "title": "Use Unity Catalog with your Lakeflow Declarative Pipelines"
  },
  {
    "url": "https://docs.databricks.com/aws/en/genie/sample-values",
    "type": "Public Preview",
    "description": "Value sampling helps Genie generate more accurate SQL by collecting and using real data values from your tables. It has two components:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-04",
    "ai_summary": "Value sampling enhances Genie's accuracy by collecting and using real data values from tables to improve SQL generation.",
    "is_new": true,
    "title": "Use value sampling to improve Genie's accuracy"
  },
  {
    "url": "https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/run-scorer-in-prod",
    "type": "Beta",
    "description": "MLflow enables you to automatically run scorers on a sample of your production traces to continuously monitor quality.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-18",
    "ai_summary": "MLflow enables automated quality assessment of production traces with predefined scorers, providing continuous monitoring and quality evaluation.",
    "is_new": true,
    "title": "Production quality monitoring (running scorers automatically)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/query/",
    "type": "Public Preview",
    "description": "This page outlines the different ways to work with your Lakebase database instance and recommends how to optimize PostgreSQL queries.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Databricks Lakebase provides methods to access and manage PostgreSQL databases, including SQL editor, notebooks, and clients, with federated query options.",
    "is_new": true,
    "title": "Access and work with a database instance"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-bricks/custom-llm",
    "type": "Beta",
    "description": "This article describes how to create a generative AI agent for custom text-based tasks usingAgent Bricks: Custom LLM.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-25",
    "ai_summary": "Create domain-specific AI agents for text-based tasks with Agent Bricks: Custom LLM, optimizing prompts and evaluation criteria without code.",
    "is_new": true,
    "title": "Use Agent Bricks: Custom LLM to create a gen AI agent for text"
  },
  {
    "url": "https://docs.databricks.com/aws/en/machine-learning/train-model/xgboost-spark",
    "type": "Public Preview",
    "description": "The Python package xgboost>=1.7 contains a new modulexgboost.spark. This module includes the xgboost PySpark estimatorsxgboost.spark.SparkXGBRegressor,xgboost.spark.SparkXGBClassifier, andxgboost.spark.SparkXGBRanker. These new classes support the inclusion of XGBoost estimators in SparkML Pipelines. For API details, see theXGBoost python spark API doc.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-06-26",
    "ai_summary": "XGBoost for SparkML Pipelines: new Python package with estimators for regression, classification, and ranking, supporting distributed training.",
    "is_new": true,
    "title": "Distributed training of XGBoost models using xgboost.spark"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dev-tools/databricks-connect/python/udf",
    "type": "Public Preview",
    "description": "This article covers Databricks Connect forDatabricks Runtime13.1 and above.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-23",
    "ai_summary": "Databricks Connect for Python supports user-defined functions (UDFs) with dependencies, serializing and deserializing UDFs for execution on Databricks compute.",
    "is_new": true,
    "title": "User-defined functions in Databricks Connect for Python"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/runtime-release-notes/5.3ml",
    "type": "Private Preview",
    "description": "Support for thisDatabricks Runtimeversion has ended. For the end-of-support date, seeEnd-of-support history. For all supportedDatabricks Runtimeversions, seeDatabricks Runtimerelease notes versions and compatibility.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-03",
    "ai_summary": "End-of-support notice for Databricks Runtime 5.3 ML, with updates on machine learning libraries and features, including MLflow integration.",
    "is_new": true,
    "title": "Databricks Runtime 5.3 ML (EoS)"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/sync-data/sync-table",
    "type": "Public Preview",
    "description": "This page describes how to create and manage a synced table. A synced table is aUnity Catalogread-only Postgres table that automatically synchronizes data from aUnity Catalogtable to your Lakebase database instance. Syncing aUnity Catalogtable into Postgres enables low-latency read queries and supports query-time joins with other Postgres tables.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Create read-only Postgres tables that synchronize data from Unity Catalog tables, enabling low-latency queries and joins with other Postgres tables.",
    "is_new": true,
    "title": "Sync data from Unity Catalog tables to a database instance"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/pretrained-models",
    "type": "Public Preview",
    "description": "Databricks includes a selection of high-quality generative AI and LLM foundation models inUnity Catalog. In addition, you can install and deploy models from external providers using Databricks Marketplace. This article describes how you can use those models and incorporate them into your inference workflows. These models allow you to access state-of-the-art AI capabilities, saving you the time and expense of building your own custom models.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-10",
    "ai_summary": "Databricks offers pre-installed AI models and marketplace access for fine-tuning and deployment, streamlining inference workflows and saving time.",
    "is_new": true,
    "title": "Get generative AI and LLM models from Unity Catalog and Marketplace"
  },
  {
    "url": "https://docs.databricks.com/aws/en/metric-views/create",
    "type": "Public Preview",
    "description": "Learn how to create a metric view to centralize business logic and consistently define key performance indicators across reporting surfaces. SeeUnity Catalogmetric views. This tutorial demonstrates how to create a metric view using the Catalog Explorer UI. To define metric views using SQL, seeCREATE VIEW.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-20",
    "ai_summary": "Create a metric view to centralize business logic and KPIs across reporting surfaces using Catalog Explorer or SQL.",
    "is_new": true,
    "title": "Create a metric view"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/configure-compute",
    "type": "Private Preview",
    "description": "This article contains instructions and considerations when configuring custom compute settings forLakeflow Declarative Pipelines.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-23",
    "ai_summary": "Configure custom compute settings for Declarative Pipelines, including cluster policies, tags, and instance types, for managed resources and performance optimization.",
    "is_new": true,
    "title": "Configure compute for Lakeflow Declarative Pipelines"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-framework/authenticate-on-behalf-of-user",
    "type": "Beta",
    "description": "While on-behalf-of user authentication is a powerful tool for enforcing secure access to sensitive data. It enables workspace users to author agents that act on behalf of other users in Databricks. During beta, it is disabled by default and must be enabled by a workspace admin. Review thesecurity considerationsfor on-behalf-of-user authentication before enabling this feature.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-10",
    "ai_summary": "On-behalf-of user authentication enables secure access to sensitive data by agents acting on behalf of end users.",
    "is_new": true,
    "title": "Deploy an agent using on-behalf-of-user authentication"
  },
  {
    "url": "https://docs.databricks.com/aws/en/ai-bi/admin/audit",
    "type": "Public Preview",
    "description": "This article has sample queries that workspace admins can use to monitor activity associated with dashboards and Genie spaces. All queries access the audit logs table, which is a system table that stores records for all audit events from workspaces in your region.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-16",
    "ai_summary": "Monitor dashboard activity in Public Preview by querying audit logs for creation, views, and access history in your workspace.",
    "is_new": true,
    "title": "Monitor AI/BI usage with audit logs and alerts"
  },
  {
    "url": "https://docs.databricks.com/aws/en/compute/configure",
    "type": "Public Preview",
    "description": "The organization of this article assumes you are using the simple form compute UI. For an overview of the simple form updates, seeUse the simple form to manage compute.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-14",
    "ai_summary": "Configure all-purpose or job compute resources in Databricks, including performance settings, runtime versions, and policies for manageability.",
    "is_new": true,
    "title": "Compute configuration reference"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/move-table",
    "type": "Public Preview",
    "description": "This article describes how to movestreaming tablesandmaterialized viewsbetween pipelines. After the move, the pipeline you move the flow to updates the table, rather than the original pipeline. This is useful in many scenarios, including:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-19",
    "ai_summary": "Move streaming tables or materialized views between pipelines, useful for splitting, merging, or updating refresh frequencies.",
    "is_new": true,
    "title": "Move tables between Lakeflow Declarative Pipelines"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dashboards/datasets/",
    "type": "Public Preview",
    "description": "This article explains how to create and manage dashboard datasets using the dataset editor in an AI/BI dashboard.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-18",
    "ai_summary": "Create and manage dashboard datasets in AI/BI dashboards using the dataset editor, define up to 100 datasets per dashboard.",
    "is_new": true,
    "title": "Create and manage dashboard datasets"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/",
    "type": "Public Preview",
    "description": "WithFoundation Model Fine-tuning(now part of Mosaic AI Model Training), you can use your own data to customize a foundation model to optimize its performance for your specific application. By conducting fine-tuning or continuing training of a foundation model, you can train your own model using significantly less data, time, and compute resources than training a model from scratch.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-28",
    "ai_summary": "Fine-tune foundation models with custom data for specific applications using Databricks, saving time and resources while retaining model control.",
    "is_new": true,
    "title": "Foundation Model Fine-tuning"
  },
  {
    "url": "https://docs.databricks.com/aws/en/tables/external-partition-discovery",
    "type": "Public Preview",
    "description": "This article describes the default partition discovery strategy forUnity Catalogexternal tables and an optional setting to enable a partition metadata log that makes partition discovery consistent withHive metastore.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-12-18",
    "ai_summary": "Unity Catalog's default partition discovery strategy is recursive, but enabling partition metadata logging improves performance for external tables with partitions.",
    "is_new": true,
    "title": "Partition discovery for external tables"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-bricks/key-info-extraction",
    "type": "Beta",
    "description": "This article describes how to create a generative AI agent for information extraction usingAgent Bricks: Information Extraction.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-25",
    "ai_summary": "Create generative AI agents for information extraction using no-code Agent Bricks: Information Extraction in Databricks with Mosaic AI support.",
    "is_new": true,
    "title": "Use Agent Bricks: Information Extraction"
  },
  {
    "url": "https://docs.databricks.com/aws/en/data-governance/unity-catalog/certify-deprecate-data",
    "type": "Beta",
    "description": "This page shows how to apply system tags toUnity Catalogsecurable objects to mark them as certified or deprecated.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-09",
    "ai_summary": "Apply 'Certified' or 'Deprecated' system tags to Unity Catalog objects for data quality and lifecycle management, governed by permissions.",
    "is_new": true,
    "title": "Flag certified and deprecated data"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-alter-table-add-constraint",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-14",
    "ai_summary": "Adds check constraints to Delta Lake tables, views, or streams with optional naming and UTF8_BINARY collation requirement.",
    "is_new": true,
    "title": "ADD CONSTRAINT clause"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dev-tools/sdk-go",
    "type": "Beta",
    "description": "In this article, you learn how to automateDatabricksoperations and accelerate development with theDatabricks SDK for Go. This article supplements theDatabricks SDK for GoREADME,API reference, andexamples.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-10-10",
    "ai_summary": "Automate Databricks operations and accelerate development with the Databricks SDK for Go, in beta, requires Go installation.",
    "is_new": true,
    "title": "Databricks SDK for Go"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/parameters",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime11.3 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-21",
    "ai_summary": "The `INFORMATION_SCHEMA.PARAMETERS` relation lists routine parameters in Unity Catalog, with columns for parameter details and metadata.",
    "is_new": true,
    "title": "PARAMETERS"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-aux-show-procedures",
    "type": "Public Preview",
    "description": "Applies to:Databricks Runtime17.0 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-22",
    "ai_summary": "List procedures with optional regex pattern filtering, returns catalog, namespace, schema, and procedure name columns.",
    "is_new": true,
    "title": "SHOW PROCEDURES"
  },
  {
    "url": "https://docs.databricks.com/aws/en/external-access/credential-vending",
    "type": "Public Preview",
    "description": "This article describes howUnity Catalogcredential vending functionality supports access to data inDatabricksfrom external processing engines.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-30",
    "ai_summary": "Databricks' Unity Catalog credential vending enables short-lived access to data from external engines with secure credentials.",
    "is_new": true,
    "title": "Unity Catalog credential vending for external system access"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/users-groups/manage-groups",
    "type": "Public Preview",
    "description": "This page explains how to manage groups for yourDatabricksaccount and workspaces.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-21",
    "ai_summary": "Manage groups for your Databricks account and workspaces, including syncing from identity providers, adding members, and updating group membership.",
    "is_new": true,
    "title": "Manage groups"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/dlt-query-history",
    "type": "Public Preview",
    "description": "This article explains how to access query histories and query profiles associated withLakeflow Declarative Pipelinesruns. You can use this information to debug queries, identify performance bottlenecks, and optimize pipeline runs.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-01-24",
    "ai_summary": "Access query histories and profiles for Lakeflow Declarative Pipelines runs to debug, optimize, and identify performance bottlenecks.",
    "is_new": true,
    "title": "Access query history for Lakeflow Declarative Pipelines"
  },
  {
    "url": "https://docs.databricks.com/aws/en/query/formats/xml",
    "type": "Public Preview",
    "description": "This article describes how to read and write XML files.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-08-09",
    "ai_summary": "Databricks supports reading and writing XML files with automatic schema inference and validation options for batch processing or streaming.",
    "is_new": true,
    "title": "Read and write XML files"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/signal-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-11",
    "ai_summary": "SIGNAL statement raises a condition or error in SQL, with optional message, argument map, and sqlstate parameters.",
    "is_new": true,
    "title": "SIGNAL statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/notebooks/notebooks-code",
    "type": "Public Preview",
    "description": "This page describes how to develop code in Databricks notebooks, including autocomplete, automatic formatting for Python and SQL, combining Python and SQL in a notebook, and tracking the notebook version history.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-25",
    "ai_summary": "Develop code in Databricks notebooks with features like autocomplete, formatting, and debugging; modularize and share code between notebooks.",
    "is_new": true,
    "title": "Develop code in Databricks notebooks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-framework/multi-agent-genie",
    "type": "Public Preview",
    "description": "This page describes Genie agent systems and shows how to create a multi-agent system using Mosaic AI Agent Framework andGenie spaces.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-03",
    "ai_summary": "Genie agents enable multi-agent systems for querying structured data and sharing answers; requires setup and authentication with Personal Access Tokens.",
    "is_new": true,
    "title": "Use Genie in multi-agent systems"
  },
  {
    "url": "https://docs.databricks.com/aws/en/security/network/serverless-network-security/pl-aws-resources",
    "type": "Public Preview",
    "description": "Effective October 7, 2024,Databricksbegan charging customers for networking costs incurred from serverless compute resources connecting to external resources. Serverless network billing is rolling out in phases, which might result in gradual billing changes. For more information on billing, seeUnderstand Databricks serverless networking costs.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-09",
    "ai_summary": "Configure private connectivity from serverless compute to AWS S3 buckets using Databricks account console UI, enabling secure access and data",
    "is_new": true,
    "title": "Configure private connectivity to AWS S3 storage buckets"
  },
  {
    "url": "https://docs.databricks.com/aws/en/delta/selective-overwrite",
    "type": "Public Preview",
    "description": "DatabricksleveragesDelta Lakefunctionality to support two distinct options for selective overwrites:",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-07-26",
    "ai_summary": "Databricks supports selective overwrites with Delta Lake functionality, offering replaceWhere and dynamic partition overwrite options for atomic updates.",
    "is_new": true,
    "title": "Selectively overwrite data with Delta Lake"
  },
  {
    "url": "https://docs.databricks.com/aws/en/archive/machine-learning/ai-onboard",
    "type": "Public Preview",
    "description": "The AI function,ai_generate_text()is deprecated. Databricks recommends usingai_query with external models.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-25",
    "ai_summary": "Databricks' AI function ai_generate_text() generates text using large language models like OpenAI and Azure OpenAI, with authentication required.",
    "is_new": true,
    "title": "Set up and considerations for ai_generate_text()"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-bricks/",
    "type": "Beta",
    "description": "This article describes whatAgent Bricksis, how it works, and the use cases it supports.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-24",
    "ai_summary": "Agent Bricks: No-code AI agent builder, automating domain-specific systems, fine-tuning models, and optimizing results for common use cases.",
    "is_new": true,
    "title": "Agent Bricks"
  },
  {
    "url": "https://docs.databricks.com/aws/en/generative-ai/agent-framework/log-agent",
    "type": "Beta",
    "description": "Log AI agents using Mosaic AI Agent Framework. Logging an agent is the basis of the development process. Logging captures a \u201cpoint in time\u201d of the agent's code and configuration so you can evaluate the quality of the configuration.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-27",
    "ai_summary": "Log AI agents using Mosaic AI Agent Framework, capturing code and configuration for evaluation and deployment.",
    "is_new": true,
    "title": "Log and register AI agents"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/row_filters",
    "type": "Public Preview",
    "description": "Databricks Runtime12.2 LTS and aboveUnity Catalogonly.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-04-18",
    "ai_summary": "Databricks Unity Catalog's INFORMATION_SCHEMA.ROW_FILTERS displays row filter metadata for relations, showing catalog, schema, table, and filter details.",
    "is_new": true,
    "title": "ROW_FILTERS"
  },
  {
    "url": "https://docs.databricks.com/aws/en/udf/python-batch-udf",
    "type": "Public Preview",
    "description": "BatchUnity CatalogPython UDFs extend the capabilities ofUnity CatalogUDFs by allowing you to write Python code to operate on batches of data, significantly improving efficiency by reducing the overhead associated with row-by-row UDFs. These optimizations makeUnity Catalogbatch Python UDFs ideal for large-scale data processing.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-04",
    "ai_summary": "Batch Unity Catalog Python UDFs enable efficient large-scale data processing with optimized performance and support for custom dependencies.",
    "is_new": true,
    "title": "Batch Python User-defined functions (UDFs) in Unity Catalog"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/user/alerts/",
    "type": "Beta",
    "description": "This page provides step-by-step guidance for enabling and usingDatabricks SQLalerts to automate query execution, evaluate custom conditions, and deliver notifications when those conditions are met. With alerts, you can proactively monitor your business data and receive timely notifications whenever reported values fall outside of expected thresholds. When you schedule an alert, its associated query runs and the alert criteria are checked-regardless of any existing schedules on the underlying query. Additionally, you can access an alert history to review the results of past alert evaluations.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-29",
    "ai_summary": "Databricks SQL alerts enable automated query execution and custom condition evaluation with timely notifications for threshold breaches.",
    "is_new": true,
    "title": "Databricks SQL alerts"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/information-schema/referential_constraints",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime10.4 LTS and aboveUnity Catalog only",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-09-16",
    "ai_summary": "Databricks SQL and Runtime (10.4+): Access referential integrity relationships between foreign keys and primary keys in Unity Catalog.",
    "is_new": true,
    "title": "REFERENTIAL_CONSTRAINTS"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/control-flow/case-stmt",
    "type": "Public Preview",
    "description": "Applies to:Databricks Runtime16.3 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-12",
    "ai_summary": "The CASE statement executes a SQL statement based on conditions, evaluating conditions in order and executing the first true result.",
    "is_new": true,
    "title": "CASE statement"
  },
  {
    "url": "https://docs.databricks.com/aws/en/oltp/create/high-availability",
    "type": "Public Preview",
    "description": "This page describes how to configure a Lakebase database instance for high availability and outlines the associated benefits.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-26",
    "ai_summary": "Configure Lakebase database instances for high availability, enabling failover and readable secondaries, with up to three secondary nodes.",
    "is_new": true,
    "title": "Configure a database instance for high availability and enable readable secondary instances"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_mask",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-06-06",
    "ai_summary": "AI Function masks entities in text using generative AI models, tuned for English, with Apache 2.0 and LLAMA licenses.",
    "is_new": true,
    "title": "ai_mask function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dlt/dbsql/materialized",
    "type": "Public Preview",
    "description": "This article describes how to create and refreshmaterialized viewsinDatabricks SQLto improve performance and reduce the cost of your data processing and analysis workloads.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-04-21",
    "ai_summary": "Create and refresh materialized views in Databricks SQL to improve performance, reduce costs, and cache query results with Unity Catalog",
    "is_new": true,
    "title": "Use materialized views in Databricks SQL"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/functions/current_recipient",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime14.2 and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-10-07",
    "ai_summary": "Retrieves recipient-specific properties in Delta Sharing, allowing data providers to control access based on custom or predefined keys.",
    "is_new": true,
    "title": "current_recipient function"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dashboards/",
    "type": "Public Preview",
    "description": "You can use dashboards to build data visualizations and share reports with your team. AI/BI dashboards feature AI-assisted authoring, an enhanced visualization library, and a streamlined configuration experience so that you can quickly transform data into sharable insights. When published, your dashboards can be shared with anyone registered to yourDatabricksaccount, even if they don't have access to the workspace. SeeShare a dashboard.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-05-29",
    "ai_summary": "Create and share data visualizations with AI-assisted authoring, enhanced library, and streamlined configuration experience for collaborative insights and reporting.",
    "is_new": true,
    "title": "Dashboards"
  },
  {
    "url": "https://docs.databricks.com/aws/en/admin/account-settings/usage",
    "type": "Public Preview",
    "description": "This article explains how to import pre-built usage dashboards to your workspaces to monitor account- and workspace-level usage.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-02-04",
    "ai_summary": "Import pre-built usage dashboards to monitor account-level usage, customize, and publish; access from Account console's Usage tab.",
    "is_new": true,
    "title": "Usage dashboards"
  },
  {
    "url": "https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/ui",
    "type": "Public Preview",
    "description": "This article describes how to create and configure a training run using theFoundation Model Fine-tuning(now part of Mosaic AI Model Training) UI. You can also create a run using the API. For instructions, seeCreate a training run using theFoundation Model Fine-tuningAPI.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2025-03-26",
    "ai_summary": "Create a Mosaic AI Model Training run using UI or API, specifying task, foundation model, training data, and advanced options.",
    "is_new": true,
    "title": "Create a training run using the Foundation Model Fine-tuning UI"
  },
  {
    "url": "https://docs.databricks.com/aws/en/sql/language-manual/data-types/timestamp-ntz-type",
    "type": "Public Preview",
    "description": "Applies to:Databricks SQLDatabricks Runtime13.3 LTS and above",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-10-10",
    "ai_summary": "Databricks SQL introduces TIMESTAMP_NTZ data type, representing timestamps without time zones, with optional year and fractional seconds support.",
    "is_new": true,
    "title": "TIMESTAMP_NTZ type"
  },
  {
    "url": "https://docs.databricks.com/aws/en/dev-tools/sqltools-driver",
    "type": "Public Preview",
    "description": "TheDatabricks Driver for SQLToolsenables you to use theSQLToolsextension forVisual Studio Codeto browse SQL objects and to run SQL queries in remoteDatabricksworkspaces.",
    "extraction_date": "2025-06-30",
    "page_last_updated": "2024-11-14",
    "ai_summary": "Databricks Driver for SQL Tools enables remote workspace connection and SQL query running in Visual Studio Code with required setup.",
    "is_new": true,
    "title": "Databricks Driver for SQLTools for Visual Studio Code"
  }
]